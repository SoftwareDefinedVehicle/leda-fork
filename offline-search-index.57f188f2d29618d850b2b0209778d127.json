[{"body":"","categories":"","description":"","excerpt":"","ref":"/leda/docs/dev-and-maintenance/bash/","tags":"","title":"Bash Utils"},{"body":"Kanto container management binds to a unix socket (default: /run/container-management/container-management.sock) and exposes a gRPC interface which can be used to obtain all the functionality of the kanto-cm cli programatically.\nThe easiest way to access this API through Rust is by creating a new Rust crate:\n$ cargo new talk-to-kanto Dependencies The most feature-rich gRPC library for Rust right now is tonic. Add the following to your Cargo.toml to make tonic and the tokio async runtime available to your crate. Tower and hyper are needed to be able to bind to the unix socket.\n[dependencies] prost = \"0.11\" tokio = { version = \"1.0\", features = [ \"rt-multi-thread\", \"time\", \"fs\", \"macros\", \"net\",] } tokio-stream = { version = \"0.1\", features = [\"net\"] } tonic = {version = \"0.8.2\" } tower = { version = \"0.4\" } http = \"0.2\" hyper = { version = \"0.14\", features = [\"full\"] } serde = { version = \"1.0.147\", features = [\"derive\"] } serde_json = { version = \"1.0.89\", default-features = false, features = [\"alloc\"] }    [build-dependencies] tonic-build = \"0.8.2\" Compiling protobufs The easiest way to obtain the kanto-cm .proto files is to add the container management repo in your project root as a git submodule:\n$ git submodule init $ git submodule add https://github.com/eclipse-kanto/container-management.git $ git submodule update --init --recursive You should now have the container-management repository available.\nTo build the .proto files during compile time, define a custom build.rs in the project root\n$ touch build.rs Add the following main function to the build.rs:\nfn main()-\u003e Result\u003c(),Box\u003cdynstd::error::Error\u003e\u003e{tonic_build::configure().build_server(false).include_file(\"mod.rs\").type_attribute(\".\",\"#[derive(serde::Serialize, serde::Deserialize)]\").compile(\u0026[\"api/services/containers/containers.proto\"],\u0026[\"container-management/containerm/\"],)?;Ok(())}Here it is important to know that tonic does not like deeply nested protobufs such as those for kanto-cm. That is why the line .include_file(\"mod.rs\") re-exports everything in a seperate module which can later be included in the main.rs file.\n\"#[derive(serde::Serialize, serde::Deserialize)]\" makes all structures (de-)serializable via serde.\nImporting generated Rust modules Now in src/main.rs add the following to import the generated Rust modules:\npubmod cm{tonic::include_proto!(\"mod\");}usecm::github::com::eclipse_kanto::container_management::containerm::api::services::containersascm_services;usecm::github::com::eclipse_kanto::container_management::containerm::api::types::containersascm_types;Now all kanto-cm services as namespaced under cm_services.\nObtaining a unix socket channel To obtain a unix socket channel:\nusetokio::net::UnixStream;usetonic::transport::{Endpoint,Uri};usetower::service_fn;letsocket_path=\"/run/container-management/container-management.sock\";letchannel=Endpoint::try_from(\"http://[::]:50051\")?.connect_with_connector(service_fn(move|_: Uri|UnixStream::connect(socket_path))).await?;This is a bit of a hack, because currently, tonic+tower don’t support binding directly to an unix socket. Thus in this case we attemp to make an http connection to a non-existent service on port 5051. When this fails, the fallback method connect_with_connector() is called where a tokio UnixStream is returned and the communication channel is generated from that.\nMaking a simple gRPC call to kanto-cm All that is left is to use the opened channel to issue a simple “list containers” request to kanto.\n// Generate a CM client, that handles containers-related requests (see protobufs) letmutclient=cm_services::containers_client::ContainersClient::new(channel);letrequest=tonic::Request::new(cm_services::ListContainersRequest{});letresponse=client.list(request).await?;Since we made all tonic-generated structures (de-)serializable we can use serde_json::to_string() to print the response as a json string.\nprintln!(\"{}\",serde_json::to_string(\u0026response)?);","categories":"","description":"","excerpt":"Kanto container management binds to a unix socket (default: …","ref":"/leda/docs/dev-and-maintenance/rust/notes-on-kanto-grpc/","tags":"","title":"Communicating with Кanto-CM via gRPC"},{"body":"","categories":"","description":"","excerpt":"","ref":"/leda/docs/dev-and-maintenance/rust/","tags":"","title":"Rust Utils"},{"body":"A general utility for monitoring the status of important sdv services/containers/devices.\nChecking the status of kanto-cm containers Kanto CM containers are split into two groups - required and optional. Both groups are checked, but only a warning is issued when an optional container is missing/not working.\nGeneral code for checking the status is:\nif [ -n \"$KANTO_CM_CONTAINERS_OPT\" ]; then \tprintf -- \"$SEPARATOR\\n\"  \tprintf -- \"${COL_WHITE}[Kanto CM Containers (OPTIONAL)]${COL_NC}\\n\" \tif [[ ${CM_STATUS} != *\"inactive\"* ]]; then \t\t# \"Optional containers\" \tKANTO_CM_LIST=$(${KANTO_CMD} list) \t# removes tabs, splits on pipe and takes the container name column ($2) \tFOUND_CONTAINERS=($(echo \"$KANTO_CM_LIST\" | awk -F'|' '{gsub(/\\t/, \"\"); print $2}')) # array with all kanto container names \t# removes tabs, splits on pipe and takes the container status colum ($4) \tFOUND_CONTAINERS_STATES=($(echo \"$KANTO_CM_LIST\" | awk -F'|' '{gsub(/\\t/, \"\"); print $4}')) # array with all kanto container states \tKANTO_CM_CONTAINERS_ARR=( $KANTO_CM_CONTAINERS_OPT )  \tfor expectedCtr in ${KANTO_CM_CONTAINERS_ARR[@]}; do \tCTR_IDX=$(get_array_element_index ${expectedCtr} ${FOUND_CONTAINERS[@]}) \tif [ ! -z $CTR_IDX ]; then \tstatus=${FOUND_CONTAINERS_STATES[$CTR_IDX]} \tif [ \"$status\" = \"Running\" ]; then \tprintf \" * %-40s : $TEXT_OK\\n\" \"${expectedCtr}\" \telse \tprintf \" * %-40s : $TEXT_WARN(%s)\\n\" \"${expectedCtr}\" \"$status\" \tfi \telse \tprintf \" * %-40s : $TEXT_WARN(%s)\\n\" \"${expectedCtr}\" \"NOT FOUND\" \tfi \tdone  \telse \tprintf \" * %-40s : $TEXT_FAIL(%s)\\n\" \"Kanto Container Management\" \"Unavailable\" \tfi  fi Here it is important to know that kanto-cm list outputs the list of containers in a different order every time it’s called. That is why, kanto-cm list is invoked once and its output is stored in a variable:\n\tKANTO_CM_LIST=$(${KANTO_CMD} list) Output:\n ID |Name |Image |Status |Finished At |Exit Code | -------------------------------------\t|-------------------------------------\t|------------------------------------------------------------\t|----------\t|------------------------------ |----------\t| d0cf1367-996d-4c83-bcc5-37d6acba9966 |feedercan |ghcr.io/eclipse/kuksa.val/databroker:0.2.5 |Running |2022-12-30T23:04:28.340430098Z |0 | f58425b0-9260-4a15-a6fc-5c59f8840839 |vehicledatabroker13 |ghcr.io/eclipse/kuksa.val/databroker:0.2.5 |Created | |0 | f8a716f7-d2c5-4165-a081-407a7acb7643 |vehicledatabroker13 |ghcr.io/eclipse/kuksa.val/databroker:0.2.5 |Stopped |2022-12-05T11:38:02.0213865Z |137 | 627f068a-b9eb-400b-826b-868f6b79e9d8 |vehicledatabroker13 |ghcr.io/eclipse/kuksa.val/databroker:0.2.5 |Running |2022-12-30T23:03:58.144873827Z |0 | 62ba3e6f-af01-4518-8393-00a5ba553062 |databroker |ghcr.io/eclipse/kuksa.val/databroker:0.2.5 |Stopped |2022-12-07T11:38:28.418199429Z |137 | 753a0b10-d72a-4065-8c27-758ed9a77c2d |vehicledatabroker13 |ghcr.io/eclipse/kuksa.val/databroker:0.2.5 |Created | |0 | So we use awk to split on pipe (column), strip unecessary tabs. print $2 then gives us the container name and print $4 - its status.\nsdv-health then proceeds to check if every container specified in the list is available and if its status is Running.\nNote: Exited is considered a fail-state.\nChecking kanto-cm socket This is a simple test -s check for the default socket path.\n","categories":"","description":"","excerpt":"A general utility for monitoring the status of important sdv …","ref":"/leda/docs/dev-and-maintenance/bash/sdv-health/","tags":"","title":"sdv-health"},{"body":"This tool is currently not available since it depends on the k3s secret-store. As of the migration to kanto-cm this functionallity was lost and the Leda team is currently developing a new secret-store solution.\n","categories":"","description":"","excerpt":"This tool is currently not available since it depends on the k3s …","ref":"/leda/docs/dev-and-maintenance/bash/sdv-provision/","tags":"","title":"sdv-provision"},{"body":"The Eclipse Leda project will provide system image “recipes” to deliver a functional Linux-based image/distribution in the context of SDV (Software Defined Vehicle), by pulling together individual contributons from the SDV and the larger OSS community.\nThe Eclipse Leda distribution will work with build scripts, package definitions, image build pipelines, etc, with the goal to pull SDV projects and dependecies from the larger OSS community into a working Linux system. Such system images (or other useful forms of delivery, as determined by the project) will be made available for consumption for anyone who is interested in working with the SDV tech stack. These deliveries take the form of container (base) images, installable/flashable image files, etc (again to be evolved by the project team according to community needs). Also in scope is concise and useful documentation for consumers of the project’s deliverables, plus a method of delivering that documentation.\nIn the context described above - the ambition of SDV to build a technology ecosystem for software-defined vehicle concern - a prime challenge will be the combination of these initially diverse components into a coherent and useful whole: all the software components in the world will not have the impact needed to transform the automotive industry unless we can make them play together coherently an form a functional portfolio. As a first step towards that goal, this proposal (Eclipse Leda) is for a “SDV distribution” project that pulls together individual contributor pieces from SDV and the larger OSS community, to deliver a functional and always-available Linux-based image/distribution with two primary goals:\n  be the crystalization point for functionally integrating diverse SDV-scope projects into a working whole\n  deliver a continually available, always working starting image for developers interested in getting going with the SDV tech stack\n  ","categories":"","description":"","excerpt":"The Eclipse Leda project will provide system image “recipes” to …","ref":"/leda/docs/about/","tags":"","title":"About Leda"},{"body":"Setting up GitHub Codespaces Git Authentication For private repositories, we need to separately authenticate against the submodule repositories, as GitHub Codespaces will only inject a token with access rights to the current repository.\n  Change to the users home directory\ncd ~   Install https://github.com/GitCredentialManager/git-credential-manager\ncurl -LO https://raw.githubusercontent.com/GitCredentialManager/git-credential-manager/main/src/linux/Packaging.Linux/install-from-source.sh \u0026\u0026 sh ./install-from-source.sh \u0026\u0026 git-credential-manager-core configure   Configure a credential store typ, e.g. git config --global credential.credentialStore plaintext\n  Verify with git config --global -l, it should show git-credential-manager-core as the credential helper.\n  Update the submodules Run git submodule update --recursive\nSee https://github.com/microsoft/vscode/issues/109050 for details.\nSetup skopeo Skopeo is needed to download various files during the build:\nsudo mkdir -p /run/containers/1000 sudo chmod a+w /run/containers/1000 skopeo login ghcr.io --authfile ~/auth.json --username \u003cyour GitHub User\u003e Enter your token when asked for the password.\nRun the build Now the actual build can be started:\ncd /workspace/leda-distro/poky source oe-init-build-env ../build-sdv-x86_64/ cd /workspace/leda-distro ./build-x86_64.sh Run qemu Start qemu with the new image:\nrunqemu qemux86-64 nographic slirp qemuparams=\"-m 2048\" Start the k3s pods Create the secret ConnectionString for the cloudconnector:\nkubectl create secret generic cloudagent \\  --from-literal=PrimaryConnectionString='\u003c\u003c\u003cConnectionString\u003e\u003e\u003e' Create the secret needed to pull images from ghcr:\nkubectl create secret docker-registry ghcr-io \\  --docker-server=ghcr.io \\  --docker-username=\u003cyour github username\u003e \\  --docker-password=\u003cyour github personal access token\u003e check the pods, you should now have them running:\nroot@qemux86-64:~# kubectl get pods NAME READY STATUS RESTARTS AGE sdv-core-vehicle-update-manager-pod 1/1 Running 1 127m mosquitto-76bcf4956f-bpncv 1/1 Running 1 127m cloud-connector 1/1 Running 18 37m Subscribe to a topic with the mosquitto in the pod: kubectl exec --stdin --tty mosquitto -- /bin/sh mosquitto_sub -v -t '#' ","categories":"","description":"","excerpt":"Setting up GitHub Codespaces Git Authentication For private …","ref":"/leda/docs/build/devenv/github-codespaces/github-codespaces-advanced/","tags":"","title":"Advanced topics"},{"body":"Note: The configuration mentioned in this chapter is already enabled in the run-leda.sh script.\nQEMU General documentation about using CAN-Bus in Qemu: https://www.qemu.org/docs/master/system/devices/can.html\nEnabling Virtual CAN Bus interfaces (vcan) No special parameters are necessary for qemu, as vcan is virtual:\nrunqemu qemux86-64 nographic slirp qemuparams=\"-m 2048\" Bring interface up:\nip link add dev vcan0 type vcan ip link set vcan0 up Enabling CAN Bus interfaces (can) Standalone CAN within Qemu To run a standalone CAN setup, qemu must be instructed to emulate a specific CAN hardware device. We will be using the kvaser_pci device in this example:\nrunqemu qemux86-64 nographic slirp qemuparams=\"-m 2048 -object can-bus,id=canbus0 -device kvaser_pci,canbus=canbus0\" After the image has booted, load the Linux Kernel Module kvaser_pci device driver and configure the CAN-Bus device (eg bitrate) before bringing the interface up:\nroot@qemux86-64:~# modprobe kvaser_pci root@qemux86-64:~# dmesg | grep kvaser [ 9.565149] kvaser_pci 0000:00:04.0: initializing device 10e8:8406 [ 9.569308] kvaser_pci 0000:00:04.0: reg_base=00000000d5a68095 conf_addr=000000002b3c7ef6 irq=20 [ 9.596942] kvaser_pci 0000:00:04.0: xilinx version=13 number of channels=0 root@qemux86-64:~# ip link show type can 4: can0: \u003cNOARP,ECHO\u003e mtu 16 qdisc noop state DOWN mode DEFAULT group default qlen 10 link/can Configure the interface:\nroot@qemux86-64:~# ip link set can0 type can bitrate 1000000 [ 165.519919] kvaser_pci 0000:00:04.0 can0: setting BTR0=0x00 BTR1=0x14 root@qemux86-64:~# ip link set can0 up [ 186.906065] IPv6: ADDRCONF(NETDEV_CHANGE): can0: link becomes ready root@qemux86-64:~# ip link show type can 4: can0: \u003cNOARP,UP,LOWER_UP,ECHO\u003e mtu 16 qdisc pfifo_fast state UP mode DEFAULT group default qlen 10 link/can Tunneling a CAN Interface from the Host runqemu qemux86-64 nographic slirp qemuparams=\"-m 2048 -object can-bus,id=canbus0 -object can-host-socketcan,id=canhost0,if=can0,canbus=canbus0 -device kvaser_pci,canbus=canbus0\" Bring interface up:\nip link add dev can0 type can ip link set can0 type can bitrate 1000000 ip link set can0 up ip link show type can Raspberry Pi CAN HAT Extensions Supported boards:\n Boards with a Microchip MCP251x based CAN chip, such as Waveshare CAN HAT or PiCAN 2  Verify driver is loaded:\n# dmesg | grep mcp [ 8.23543] mcp251x spi0.0 can0: MCP2515 successfully initialized Verify SocketCAN network interface shows up:\n# ip link show type can 3: can0: \u003cNOARP,ECHO\u003e mtu 16 qdisc noop state DOWN mode DEFAULT roup default qlen 10 Continue with configuring the CAN chip and bring up the SocketCAN network interface:\n# ip link set can0 type can bitrate 1000000 # ip link set can0 up # ip link show type can Linux Kernel Modules The following Linux Kernel modules are available on the quickstart images:\nNote: For QEMU, only kvaser_pci is used\nLeda main Kernel  peak_pciefd - Socket-CAN driver for PEAK PCAN PCIe/M.2 FD family cards m_can - CAN bus driver for Bosch M_CAN controller m_can_pci - CAN bus driver for Bosch M_CAN controller on PCI bus m_can_platform - M_CAN driver for IO Mapped Bosch controllers softing - Softing DPRAM CAN driver cc770_platform - Socket-CAN driver for CC770 on the platform bus cc770_isa - Socket-CAN driver for CC770 on the ISA bus cc770 - cc770CAN netdevice driver ifi_canfd - CAN bus driver for IFI CANFD controller kvaser_usb - CAN driver for Kvaser CAN/USB devices etas_es58x - Socket CAN driver for ETAS ES58X USB adapters ucan - Driver for Theobroma Systems UCAN devices peak_usb - CAN driver for PEAK-System USB adapters kvaser_pciefd - CAN driver for Kvaser CAN/PCIe devices kvaser_pci - Socket-CAN driver for KVASER PCAN PCI cards f81601 - Fintek F81601 PCIE to 2 CANBUS adaptor driver sja1000_isa - Socket-CAN driver for SJA1000 on the ISA bus plx_pci - Socket-CAN driver for PLX90xx PCI-bridge cards with the SJA1000 chips sja1000 - sja1000CAN netdevice driver ems_pci - Socket-CAN driver for EMS CPC-PCI/PCIe/104P CAN cards peak_pci - Socket-CAN driver for PEAK PCAN PCI family cards sja1000_platform - Socket-CAN driver for SJA1000 on the platform bus vxcan - Virtual CAN Tunnel c_can_platform - Platform CAN bus driver for Bosch C_CAN controller c_can - CAN bus driver for Bosch C_CAN controller c_can_pci - PCI CAN bus driver for Bosch C_CAN/D_CAN controller slcan - serial line CAN interface can_dev - CAN device driver interface vcan - virtual CAN interface can-isotop - PF_CAN isotp 15765-2:2016 protocol can-gw - PF_CAN netlink gateway can-j1939 - PF_CAN SAE J1939 can-raw - PF_CAN raw protocol can-bcm - PF_CAN broadcast manager protocol can - Controller Area Network PF_CAN core  Raspberry Pi The following Linux Kernel modules are available on the quickstart image for Raspberry Pi:\n can - Controller Area Network PF_CAN core vxcan - Virtual CAN Tunnel can-dev - CAN device driver interface can-bcm - PF_CAN broadcast manager protocol can-gw - PF_CAN netlink gateway can-raw - PF_CAN raw protocol can-isotop - PF_CAN isotp 15765-2:2016 protocol can-j1939 - PF_CAN SAE J1939 vcan - virtual CAN interface slcan - serial line CAN interface mcp251x - Microchip 251x/25625 CAN driver mcp251xfd - Microchip 251xFD Family CAN controller driver ems_usb - CAN driver for EMS Dr. Thomas Wuensche CAN/USB interfaces gs_usb - Socket CAN device driver for Geschwister Schneider UG peak_usb - CAN driver for PEAK-System USB adapters  ","categories":"","description":"","excerpt":"Note: The configuration mentioned in this chapter is already enabled …","ref":"/leda/docs/general-usage/running-qemu/canbus/","tags":"","title":"CAN Bus"},{"body":"Setting up Development Environment in GitHub Codespaces Install the GitHub Codespaces Extension Note: When using our DevContainer, the GitHub Codespaces extension is pre-installed.\n Start VSCode Go to Extensions Search for “GitHub Codespaces” Click Install  Alternatively, create a new codespace via the GitHub web interface:\nSelect a big enough machine type for Yocto/BitBake, e.g. 16 CPU. You need at leasst 50GB disk space.\nBuilding Leda in a Github Codespace After successfully obtaining and connecting to a codespace you can build Leda either with kas or manually:\n  To build with kas follow the instructions at: Building with kas\n  To build manually: Building manually\n  Private Repositories When using GitHub Codespaces with submodules and private repositories, a separate tool for git authentication is required (see VSCode issue #109050), as the authentication token provided to the GitHub Codespaces virtual machine only allows access to the main repository.\nGit Credential Manager: https://aka.ms/gcm\nInstallation:\ncurl -LO https://raw.githubusercontent.com/GitCredentialManager/git-credential-manager/main/src/linux/Packaging.Linux/install-from-source.sh \u0026\u0026 sh ./install-from-source.sh \u0026\u0026 git-credential-manager-core configure ","categories":"","description":"","excerpt":"Setting up Development Environment in GitHub Codespaces Install the …","ref":"/leda/docs/build/devenv/github-codespaces/","tags":"","title":"Codespaces"},{"body":"Thanks for your interest in this project and in our community.\n https://projects.eclipse.org/projects/automotive.leda  Contact Contact the project developers via the project’s “dev” list.\n https://accounts.eclipse.org/mailing-list/leda-dev  Developer resources Information regarding source code management, builds, coding standards, and more.\n https://projects.eclipse.org/projects/automotive.leda/developer  The project maintains the following source code repositories\n https://github.com/eclipse-leda/leda https://github.com/eclipse-leda/leda-distro https://github.com/eclipse-leda/meta-leda  Publications  Eclipse Leda Introduction Video - YouTube, 2min Eclipse SDV - First Contribution Day (June 2022) - Session Recording, YouTube, 26min Eclipse SDV - Second Contribution Day (September 2022) - Session Recording, YouTube, 30min Eclipse Leda - Slides SDV Contribution Day - September 2022 - Slides (PDF)  ","categories":"","description":"","excerpt":"Thanks for your interest in this project and in our community. …","ref":"/leda/docs/project-info/community/","tags":"","title":"Community"},{"body":"BitBake and Metalayers The example build configurations in this repository are based on the official BitBake Quickstart tutorial and have been extended to include SDV components.\nTo set up your own BitBake build configuration, follow the BitBake documentation and include meta-leda in your bblayers.conf and add the SDV packages into your local.conf. You may use the the local.conf files in the build-sdv-*/conf/local.conf as a guiding reference.\nSDV Meta Layer The meta-leda layer conatins the BitBake Classes and Recipes to integrate SDV Components into a BitBake based build setup.\nPlease see https://github.com/eclipse-leda/meta-leda for more information.\nRecipes for Kubernetes Pods The SDV stack is based on a Kubernetes Control Plane and the intention is to have as much components containerized as possible, to ensure a high degree of isolation and updateability.\nTo automatically deploy the pods of the SDV reference implementation and example applications and services, the build configurations will deploy a couple of Pod specifiction files into the auto-deployment folder of k3s.\nAt start time of k3s, these pods will be automatically deployed:\n Cloud Connector by Eclipse Kanto Log and Trace by OpenTelemetry Example Application: Seat Adjuster python app Vehicle API  Data Broker Example Seat Service (CAN-bus implementation)   Vehicle Update Manager  For a full list of pods, see meta-leda/recipes-sdv/sdv-core/files/sdv-core-bundle-pods/\nRecipes for containerized applications OpenEmbedded’s meta-virtualization already contains some recipes and reusabled classes for building virtualization and containerized applications.\nmeta-leda extends that functionality by using skopeo to package container images. To minimize the runtime requirements (dependencies, disk usage), an approach to pre-load container images and its layers directly into the content storage of the container runtime is followed.\n","categories":"","description":"","excerpt":"BitBake and Metalayers The example build configurations in this …","ref":"/leda/docs/build/concepts/","tags":"","title":"Concepts"},{"body":"Initial configuration  To initialize DAPR, run:  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml /dapr init -k  For accessing private container registries, create /etc/rancher/k3s/registries.yml: Example for accessing GitHub Packages container registry at ghcr.io:  configs: \"ghcr.io\": auth: username: \u003c\u003cgithub-username\u003e\u003e password: \u003c\u003cgithub-personal-access-token\u003e\u003e Note: Ensure the Personal Access Token has the permission to access GitHub packages\n Get your Azure IoT Hub Connection String and base64 it:  echo \"\u003c\u003cAzureIoTHubConnectionString\u003e\u003e\" | base64 -w 0 Put the encoded result into /var/lib/rancher/k3s/server/manifest/cloud-connector.secret.yaml (rename from template)\n Verify that credentials have been crated: kubectl get secrets cloudagent Restart k3s: systemctl restart k3s Manually start the k3s agent:  k3s agent -s https://10.0.2.15:6443 -lb-server-port 6445 -t $(cat /var/lib/rancher/k3s/server/node-token) ","categories":"","description":"","excerpt":"Initial configuration  To initialize DAPR, run:  export …","ref":"/leda/docs/build/misc/dapr-init/","tags":"","title":"DAPR Installation"},{"body":"Go to the Eclipse Leda Releases page and download the disk image for the respective machine and the respective Linux kernel:\nLatest Release Artifacts Note: There are no official releases yet. The artifacts available on the Release page are for testing the build and release workflows and should be considered as unstable nightly builds from the main branch.\n   Machine Filename Description     QEMU x86_64 eclipse-leda-qemu-x86_64.tar.xz For running QEMU x86 64-Bit   QEMU ARM 64 eclipse-leda-qemu-arm64.tar.xz For running QEMU ARM 64-Bit   Raspberry Pi 4 eclipse-leda-raspberrypi.tar.xz For running on Raspberry Pi 4 (SD-Card Image)    Using GitHub CLI tool To download all files of the latest release using the GitHub CLI:\n Install GitHub CLI, e.g. for Ubuntu:  curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | sudo tee /etc/apt/sources.list.d/github-cli.list \u003e /dev/null sudo apt update sudo apt install gh  Authenticate to GitHub:  gh auth login  Download Leda latest release:  On Linux:\nmkdir leda \u0026\u0026 cd leda gh release download \\ --pattern '*.zip' \\ --pattern 'eclipse-leda-*' \\ --repo eclipse-leda/leda-distro On Windows:\ngh release download --pattern \"*.zip\" --pattern \"eclipse-leda-*\" --repo eclipse-leda/leda-distro  Continue with Running Eclipse Leda on QEMU or Running Eclipse Leda on Raspberry Pi 4  ","categories":"","description":"","excerpt":"Go to the Eclipse Leda Releases page and download the disk image for …","ref":"/leda/docs/general-usage/download-releases/","tags":"","title":"Download latest release"},{"body":" Publish/Subscribe messaging infrastructure for cloud connectivity by Eclipse Kanto  local messaging for applications and SDV system components via an MQTT message broker connection to a backend messaging hub, such as Azure IoT Hub or the IoT Suite device identification and authentication for cloud connectivity by using TLS device certificates   Linux container runtime and Kubernetes control plane  a Kubernetes-compliant control plane to orchestrate worker nodes, services, container pods and deployments. k3s as the current implementation of the control plane and containerd.io as the default container runtime. Both layers of container runtimes can be exchanged with other implementations   A Vehicle Update Manager to orchestrate deployments of Vehicle Applications, configurations and base operating system updates via Kubernetes-standardized protocol specifications An example Vehicle Seat Service implementation to showcase  the Eclipse Velocitas programming model, the Eclipse Kuksa.VAL vehicle databroker and the Covesa Vehicle Signal Specification the communication with basic vehicle communication networks such as CAN-Bus (CAN Feeder)   A Self Update Agent for firmware-over-the-air (FOTA) updates, using an A/B deployment strategy  Integration with RAUC   An OpenTelemetry collector and example configurations to collect and publish logs and metrics of containerized Vehicle Applications to the cloud backend for further processing  The features of the reusable build recipes implemented as an OpenEmbedded metalayer meta-leda are:\n Build recipes for a Yocto-based distribution to build SDV-related software components Build recipes for customizations of the target device’s storage structure to enable A/B system updates Build recipes for pre-packaging container images into the device during the manufacturing process to minimize initial online provisioning time A customized minimal setup for use on constrained devices and a full setup with convenient developer tools Ready images for virtual devices, for automated testing and evaluation purposes, eg QEMU ARM-64 Ready images for physical devices, for evaluation and demo purposes, eg Raspberry Pi 4  ","categories":"","description":"","excerpt":" Publish/Subscribe messaging infrastructure for cloud connectivity by …","ref":"/leda/docs/about/features/","tags":"","title":"Features"},{"body":"Create a new GitHub Runner for this repo Start with creating a new azure VM:\n Ubuntu Server Latest, currently 20.04 Size Standard D16ds v5 The admin user should be called “runner”  Once the VM is ready:\n Stop the VM Go to “Disk” and resize the OS disk to 512 GB Start the VM again  Run the script to setup the runner Log on to the VM as runner. Either copy the scripts/PrepVMasGHRunner.sh onto the VM or create a new script:\nnano prep.sh Copy the content of the PrepVMasGHRunner.sh from this repo into the new file, save it and make it executable:\nchmod 755 prep.sh Call it with the token and the next available nummer, see below how to get this items:\n./prep.sh \"ASYVOMU........DTCFCMBA\" 3 In the Azure portal go the VM, go to the “network” section and delete the rule opening port 22. Congratulation, you are done!\nHow to get the token and the number to call the script In the repo, go to “Settings” -\u003e “Actions”. You see the currently provisioned runners:\nPick the next number and pass it to the script.\nTo get the token press the green button in the above screenshot. The token is in the command:\n","categories":"","description":"","excerpt":"Create a new GitHub Runner for this repo Start with creating a new …","ref":"/leda/docs/build/devenv/github-runner/","tags":"","title":"GitHub Runner"},{"body":"The project aims to provide an integration point for Open Source components for the Software Defined Vehicle. For vehicle software systems, there are a lot of requirements to consider. Some of these requirements are taken into account for Leda’s quickstart setups, thereas some other requirements can only be met once in a production environment and by customizing the target device image.\nThe following document will list some of these requirements and give an explanation on why they are met in the Leda quickstart distribution.\nOverview  Provide an example operating system and configuration for constrained in-vehicle devices Integrate software-defined-vehicle Open Source components to showcase the available features and their state of maturity Demonstrate the use and interaction of open protocols and specifications, such as  the Kubernetes specifications OpenTelemetry specs and components Eclipse IoT related specifications for software rollouts and digital twin representations open specifications from the The Connected Vehicle Systems Alliance (COVESA)    Background Information    Requirement Met Context information     Footprint Constraints Yes Embedded in-vehicle devices have constrained resources such as CPU power, disk space, memory etc. Desktop operating systems use as much space as possible, thereas in-vehicle operating systems are as small as possible, omitting a lot of unnecessary tooling such as package managers or convenience tools for administrators, documenation (man pages) etc. Leda provides two different types of operating system images: one for quickstart and demo cases with some tooling, and a another minimal image which only contains the necessary software components for remote usage. \nSDV Rescue Image is about 130MB\nSDV Minimal Image is about 300MB SDV Full Image (with convenience tools and pre-cached containers) is about 900MB\nNote: Future roadmap includes more footprint optimizations   Hardened system Partly A vehicle device and its operating system are vulnerable targets in the same way as any other computer, especially once they are connected to the internet. Classically, such in-vehicle devices were hardened down once with additional audit logging for any kind of network traffic. In a connected world, these devices now become mostly always-on devices and behave like any other Internet of Things devices. Special care is to be taken to guard them against vulnerabilities by keeping these devices up-to-date, establish continuous automated monitoring and other methodologies known from DevSecOps.\nAs Leda is supposed to be used for quickly starting and evaluating SDV software, we balance these type of requirements. For example, installing system updates in Leda is only possible with signed update bundles, but the demo uses a self-signed certificate. For production use, these self-signed certificates would of course be replaced by the manufacturer’s real certificates.         ","categories":"","description":"","excerpt":"The project aims to provide an integration point for Open Source …","ref":"/leda/docs/about/goals/","tags":"","title":"Goals"},{"body":"Baseline (no apps installed) Baseline: 400 MB Baseline after k3s started: 535 MB disk\nInstalled components:\n Container Management: runc, containerd, k3s System Services: systemd, openssh, mosquitto, mosquitto-clients No dapr No cloud connector or edgecontainerd yet. No self update agent No containers, no vehicle apps etc.  root@qemux86-64:/bin# df -h Filesystem Size Used Avail Use% Mounted on /dev/root 2.5G 506M 1.8G 22% /  Memory Usage: 200MB  root@qemux86-64:/var/lib/rancher/k3s/server# free total used free shared buff/cache available Mem: 1.9G 329.6M 1.4G 16.6M 222.2M 1.6G Swap: With dapr and pods With DAPR preinstalled, no pods yet root@qemux86-64:~# df -h Filesystem Size Used Avail Use% Mounted on /dev/root 2.5G 1.3G 1005M 57% / root@qemux86-64:~# free -h total used free shared buff/cache available Mem: 1.9G 812.7M 19.4M 18.1M 1.1G 1.1G Swap: 0 0 0 With DAPR, with cloud connector and vehicle update manager root@qemux86-64:~# df -h Filesystem Size Used Avail Use% Mounted on /dev/root 2.5G 2.2G 90M 97% / root@qemux86-64:~# free -h total used free shared buff/cache available Mem: 1.9G 849.5M 100.8M 26.6M 1.0G 1.1G Swap: 0 0 0 Details Dependencies\n k3s (binary executable only): 140 MB cni (container networking): 51 MB containerd-ctr: 26 MB containerd: 46 MB dapr cli: 38 MB helm cli: 43 MB runc: 10 MB  SDV Components\n vehicle-data-cli: 2.3 MB (dynamic)  Medium:\n Linux Kernel (Poky minimal): 8 MB oci-image-tool: 9 MB oci-runtime-tool: 7 MB sshd: 1 MB libstdc++: 2 MB lic: 2 MB libcrypto: 3 MB containerd-shim: 7 MB containerd-shim-runc-v1: 9 MB containred-shim-runc-v2: 9 MB libsystemd: 1 MB busybox: 1 MB  ","categories":"","description":"","excerpt":"Baseline (no apps installed) Baseline: 400 MB Baseline after k3s …","ref":"/leda/docs/build/misc/diskusage/","tags":"","title":"Resource Consumptions"},{"body":" Initial Open Source contribution expected by Q2 2022 (Done) A first milestone build is expected end of 2022 Plan for the first release cycle to be created in Q1/2023 Release cycles are planned every 3-6 months Release planning will be conducted together with corresponding Eclipse projects  Future Work The project intends to be the integration and collaboration platform for Software defined Vehicle functionality.\nExemplary future work:\n Migrate to official Eclipse Kanto releases for Cloud Connector, Container Management and Vehicle Update Manager Include reference implementations from the Eclipse Software Defined Vehicle working group projects:  Eclipse Velocitas Eclipse Kuksa Eclipse SommR Eclipse Chariott Eclipse Backend function Bindings (BfB)   Include and showcase more features regarding development, operation and monitoring of Vehicle Services and Vehicle Applications  If you have feedback, please use GitHub Issues\n","categories":"","description":"","excerpt":" Initial Open Source contribution expected by Q2 2022 (Done) A first …","ref":"/leda/docs/about/roadmap/","tags":"","title":"Roadmap"},{"body":"If you want to execute the image without building first, grab the latest release or build artifacts from https://github.com/eclipse-leda/leda-distro/\nRecommendations  A Linux host with 8 vCPUs, 16GB of RAM and SSD storage is recommended Your Linux user should be sudoer to allow TAP network interfaces to be set up  QEMU x86_64   Install Qemu https://www.qemu.org/, e.g. for Ubuntu:\nsudo apt-get update -y sudo apt-get install -y xz-utils qemu-system-x86-64    Download latest Eclipse Leda release\n  Uncompress the archive\ntar xf eclipse-leda-qemu-x86_64.tar.xz    Run QEMU on Linux:\n./run-leda.sh    Run QEMU on Windows:\nrun-leda.cmd    Login as root without password on login prompt\n  Verify and wait until k3s is started: systemctl status k3s   Optional: Check the system health: sdv-health Note: The status of some pods and the cloud connector are expected to stay in FAILED status as long as the Device Provisioning steps are not completed.\n  Continue with Device Provisioning\n  QEMU ARM 64-Bit   Install Qemu https://www.qemu.org/, e.g. for ARM 64-Bit: sudo apt install qemu-system-aarch64\n  Download latest Eclipse Leda release\n  Uncompress the archive\ntar xf eclipse-leda-qemu-arm64.tar.xz    Run QEMU on Linux:\n./run-leda.sh    Run QEMU on Windows:\nrun-leda.cmd    Login as root without password on login prompt\n  Verify and wait until k3s is started: systemctl status k3s\n  Optional: Check the system health: sdv-health\nNote: The status of some pods and the cloud connector are expected to stay in FAILED status as long as the Device Provisioning steps are not completed.\n  Continue with Device Provisioning\n  QEMU ARM 32-Bit   Install Qemu https://www.qemu.org/, e.g. for ARM 32-Bit: sudo apt install qemu-system-arm\n  Download latest Eclipse Leda release\n  Uncompress the archive\ntar xf eclipse-leda-qemu-arm.tar.xz    Run QEMU on Linux:\n./run-leda.sh    Run QEMU on Windows:\nrun-leda.cmd    Login as root without password on login prompt\n  Verify and wait until k3s is started: systemctl status k3s\n  Optional: Check the system health: sdv-health\nNote: The status of some pods and the cloud connector are expected to stay in FAILED status as long as the Device Provisioning steps are not completed.\n  Continue with Device Provisioning\n  ","categories":"","description":"","excerpt":"If you want to execute the image without building first, grab the …","ref":"/leda/docs/general-usage/running-qemu/","tags":"","title":"Running on QEMU"},{"body":"What you need:\n A Raspberry Pi 4 with 2 GiB of RAM or more, recommended is 8 GiB Network connection (Ethernet or Wifi) with transparent internet access Optional keyboard and display (makes it easier to troubleshoot)  Steps:\n  Download the latest released SD-Card Image: eclipse-leda-raspberrypi.tar.xz\n  Uncompress the SD Card image:\napt-get install -y xz-utils tar xf eclipse-leda-raspberrypi.tar.xz bzip2 -d -f sdv-image-full-raspberrypi4.wic.bz2    Flash the sdv-image-all-raspberrypi4.wic file to an SD-Card\n On Linux:  Install bmap tools: sudo apt-get install -y bmap-tools Insert SD Card and check which device is mounted: sudo fdisk -l Unmount the device: sudo umount /dev/sd[X] sudo bmaptool copy --bmap sdv-image-all-raspberrypi4-64.wic.bmap sdv-image-all-raspberrypi4-64.wic.gz /dev/sd[X] Note: Using bmap is faster but works the same as with plain dd if=\u003cwic-file\u003e of=dev/sd[x].   On Windows:  Raspberry Pi Imager Balena Etcher      Optional: If you need to adapt the network configuration eg Wifi credentials, edit the configuration files on the boot partition.\n  Shutdown the Raspberry and insert the SD-Card into the Raspberry Pi SD-Card slot at the bottom\n  Power on your Raspberry to boot the image\n  Login with root\n  Check disk space:\n  The raspberry-growdisk system service will do this automatically on first boot.\n  To manually enlarge the available disk space on the SD-Card, resize the disk partition: parted /dev/mmcblk0 resizepart 6 100% \u0026\u0026 resize2fs /dev/mmcblk0p6. Verify with df -h.\nNote: Due to changes in the disk partition, the partition number (6 in the example) may have changed.\n    Verify and wait until k3s is started: systemctl status k3s\n  Optional: Check the system health: sdv-health\n  Continue with Provisioning Raspberry Pi\n  ","categories":"","description":"","excerpt":"What you need:\n A Raspberry Pi 4 with 2 GiB of RAM or more, …","ref":"/leda/docs/general-usage/raspberry-pi/","tags":"","title":"Running on Raspberry Pi"},{"body":"This chapter describes the steps necessary to perform a local (without cloud) self update of the operating system.\nSelf-Update using RAUC Update Bundles   On host: Update bundle sdv-rauc-bundle-qemux86-64.raucb is in current folder\nNote: In the development environment, the update RAUC Update Bundle is located in the BitBake machine-specific output folder, for example tmp/deploy/images/qemux86-64\n  On host: Start a dummy web server for serving the update file\npython3 -m http.server --bind 192.168.7.1    On host: open two new terminals - one for monitoring and one for triggering the self-update\n  Terminal 1: To view the progress, watch the MQTT topics selfupdate/desiredstate and selfupdate/desiredstatefeedback:\nmosquitto_sub -h 192.168.7.2 -p 31883 -t \"selfupdate/#\"    Terminal 2: Trigger the actual self update process by publishing an MQTT message to selfupdate/desiredstate:\nmosquitto_pub -h 192.168.7.2 -p 31883 -t \"selfupdate/desiredstate\" -f start-update-example.yaml      Switch to a terminal in the guest\n  On guest: After the self update process completed, check the status:\nrauc status --detailed    Self-Update Trigger Message start-update-example.yaml file:\napiVersion:\"sdv.eclipse.org/v1\"kind:SelfUpdateBundlemetadata:name:self-update-bundle-examplespec:bundleName:swdv-qemux86-64-build42bundleVersion:v1beta3 bundleDownloadUrl:http://192.168.7.1:8000/sdv-rauc-bundle-qemux86-64.raucbbundleTarget:base ","categories":"","description":"","excerpt":"This chapter describes the steps necessary to perform a local (without …","ref":"/leda/docs/device-provisioning/self-update/self-update-tutorial/","tags":"","title":"Self Update Tutorial"},{"body":"Sharing a directory with the guest When you want to copy files between the host and the guest, an easy way is to use an SFTP tunnel. With sshfs, you can mount a local directory to a remote directory via SSH.\nPre-Requisites Installation of needed packages:\n Run apt-get install sshfs on your host Enable CORE_IMAGE_EXTRA_INSTALL += \" openssh-sftp-server\" in local.conf of your image Verify SFTP connection working with sftp -P 2222 root@localhost  Transfering files from host to guest When you want to copy files from the host to the guest, an easy way is to use an SFTP tunnel. With sshfs, you can mount a local directory to a remote directory via SSH.\n Create a mount point on your host: mkdir remote Open the SSH Filesystem tunnel: sshfs root@localhost:/ remote/ -p 2222 Check files: ls -al remote/ - you should see the root filesystem of the device now You can now easily copy files: cp foo.txt remote/home/root/  Transfering files from guest to host Note: The reverse direction, e.g. initiating an SSH tunnel from within the device to the host, is currently not supported by the installed software on the image.\n","categories":"","description":"","excerpt":"Sharing a directory with the guest When you want to copy files between …","ref":"/leda/docs/general-usage/running-qemu/filetransfer/","tags":"","title":"Transferring Files"},{"body":"SDV Core Utils The quickstart image contains the following toolings:\n SDV Device Info SDV Health CAN Forwarder SDV MOTD  For details, please see leda-utils\nConnecting to Mosquitto If you want to use your favourite MQTT tool with the quickstart image, you can connect to the MQTT broker running on the device by using exposed port 31883:\n$# mosquitto_sub -h 192.168.7.2 -p 31883 -t '#' ","categories":"","description":"","excerpt":"SDV Core Utils The quickstart image contains the following toolings: …","ref":"/leda/docs/build/misc/tools/","tags":"","title":"Utilities"},{"body":"Validating the release Steps to validate if a release is properly working:\n  Create a new pre-release from your branch\n  Download the release artifacts onto a clean system.\nDo not use your build environment, to minimize the impact of existing environment configuration from BitBake etc.\n  Run the run-leda scripts to execute Qemu\nNote: You should test each of the release archives, for each target machine.\n  Follow the Device Provisioning guide\n  Perform some verification tests (see below)\n  Cleanup: Delete the pre-release and the git tag:\ngit push --delete origin \u003ctagname\u003e    Ideas for manual verification steps Note: These are just for manual testing, as we intend to extend the automated tests as much as possible.\n Operating system level  Run sdv-health on the shell Verify disk partitions and RAUC status, e.g. rauc status Verify network interface and CAN-Bus with ip addr   Kubernetes  Check status of cluster and pods with k9s    ","categories":"","description":"","excerpt":"Validating the release Steps to validate if a release is properly …","ref":"/leda/docs/build/release/validation/","tags":"","title":"Validating"},{"body":"Preparation  Obtain the Docker Engine for your distribution and add your non-privileged user to the docker group (sudo usermod -aG docker $USER ) Install Visual Studio Code  Visual Studio Code: Development Containers  Open Visual Studio Code Open Command Palette (F1) and select Clone repository in Container Volume Select eclipse-leda/meta-leda and the main branch. Adapt proxy configurations if necessary (.devcontainer/proxy.sh)  For a clean remote build machine, you may want to set up a development environment on GitHub CodeSpaces\nBuilding Leda in a VSCode DevContainer: After successfully setting up your DevContainer you can build Leda either with kas or manually:\n  To build with kas follow the instructions at: Building with kas\n  To build manually: Building manually\n  Visual Studio DevContainer Setup Authentication The build process requires online connection and you must be authenticated to access private repositories.\n Create a GitHub Personal Access Token (PAT) at https://github.com/settings/tokens and grant read:packages permission Use Configure SSO and authorize your PAT for the organization On the build host, authenticate to ghcr.io: skopeo login ghcr.io --authfile ~/auth.json --username \u003cusername\u003e and enter the PAT as password  You may need to create the folder where skopeo is storing authentication information beforehand:  sudo mkdir -p /run/containers/1000 sudo chmod a+w /run/containers/1000  Start the bitbake build process  ","categories":"","description":"","excerpt":"Preparation  Obtain the Docker Engine for your distribution and add …","ref":"/leda/docs/build/devenv/vscode-devcontainer/","tags":"","title":"VSCode DevContainer"},{"body":"The vehicle update manager container requires the following configuration:\n  Container needs to run in privileged mode to enable automatic reboot.\nNote: This is enabled by default on the Leda Quickstart images to simplify automated testing.\n  Connection to MQTT broker, defaults to THINGS_CONN_BROKER=tcp://mosquitto:1883\n  Enable container orchestration feature: THINGS_FEATURES=ContainerOrchestrator\n  Optional configuration options are:\n SELF_UPDATE_ENABLE_REBOOT=true Enable automatic reboot after a successfull application of the update bundle. SELF_UPDATE_TIMEOUT=30m Timeout for downloading and installing an update bundle.  Example Deployment Specification apiVersion: v1 kind: ServiceAccount metadata: name: vehicle-update-manager --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: vehicle-update-manager rules: - apiGroups: - '*' resources: - '*' verbs: - '*' --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: vehicle-update-manager subjects: - kind: ServiceAccount name: vehicle-update-manager namespace: default roleRef: kind: ClusterRole name: vehicle-update-manager apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: name: vehicle-update-manager spec: selector: matchLabels: component: vehicle-update-manager template: metadata: labels: component: vehicle-update-manager spec: serviceAccountName: vehicle-update-manager containers: - name: vehicle-update-manager image: \u003crepository\u003e/vehicleupdatemanager:\u003ctag\u003e imagePullPolicy: IfNotPresent securityContext: privileged: true env: - name: SELF_UPDATE_TIMEOUT value: 30m - name: SELF_UPDATE_ENABLE_REBOOT value: \"true\" - name: THINGS_CONN_BROKER value: tcp://mosquitto:1883 - name: THINGS_FEATURES value: ContainerOrchestrator volumeMounts: - mountPath: /proc name: proc volumes: - hostPath: path: /proc name: proc imagePullSecrets: - name: ghcr-io ","categories":"","description":"","excerpt":"The vehicle update manager container requires the following …","ref":"/leda/docs/device-provisioning/vehicle-update-manager/vehicle-update-manager-configuration/","tags":"","title":"Configuration"},{"body":"TLDR: To deploy a container in the final Leda image, all you generally need to do is add the manifest in the kanto-containers directory and re-build.\nKanto-CM does not provide (currently) an out-of-the box feature that allows for the automatic deployment of containers through manifest files similar to k3s’ automated deployment of k8s-manifests found in the /var/lib/rancher/k3s/server/manifests directory.\nThis can be worked around via a bash script for each container that runs on boot and makes sure it’s deployed. Even though this approach is functional it is not very structured and would require a lot repeating code.\nThat is why the “Kanto Auto deployer” tool was developed. It directly implements the ideas in Communicating with Кanto-CM via gRPC.\nThe compiled binary takes a path to a directory containing the json manifests, parses them into Rust structures and sends gRPC requests to kanto container management to deploy these containers. If the container is already deployed the manifest is ignored.\nManifest structure Kanto auto deployer uses the exact same structure for its manifests as the internal representation of container state in kanto container management. For example:\n{  \"id\": \"\",  \"name\": \"databroker\",  \"image\": {  \"name\": \"ghcr.io/eclipse/kuksa.val/databroker:0.2.5\",  \"decrypt_config\": null  },  \"host_name\": \"\",  \"domain_name\": \"\",  \"resolv_conf_path\": \"\",  \"hosts_path\": \"\",  \"hostname_path\": \"\",  \"mounts\": [],  \"hooks\": [],  \"host_config\": {  \"devices\": [],  \"network_mode\": \"bridge\",  \"privileged\": false,  \"restart_policy\": {  \"maximum_retry_count\": 0,  \"retry_timeout\": 0,  \"type\": \"unless-stopped\"  },  \"runtime\": \"io.containerd.runc.v2\",  \"extra_hosts\": [],  \"port_mappings\": [],  \"log_config\": {  \"driver_config\": {  \"type\": \"json-file\",  \"max_files\": 2,  \"max_size\": \"100M\",  \"root_dir\": \"\"  },  \"mode_config\": {  \"mode\": \"blocking\",  \"max_buffer_size\": \"\"  }  },  \"resources\": null  },  \"io_config\": {  \"attach_stderr\": false,  \"attach_stdin\": false,  \"attach_stdout\": false,  \"open_stdin\": false,  \"stdin_once\": false,  \"tty\": false  },  \"config\": null,  \"network_settings\": null,  \"state\": {  \"pid\": -1,  \"started_at\": \"\",  \"error\": \"\",  \"exit_code\": 0,  \"finished_at\": \"\",  \"exited\": false,  \"dead\": false,  \"restarting\": false,  \"paused\": false,  \"running\": false,  \"status\": \"\",  \"oom_killed\": false  },  \"created\": \"\",  \"manually_stopped\": false,  \"restart_count\": 0 } The only difference to the actual internal state representation is that fields in the manifest can be left empty (\"\") if they are not important for the deployment. These values will be filled in with defaults by kanto-cm after deployment.\nFor example, you do not need to specify the container “id” in the manifest, as an unique uuid would be assigned automatically after deployment.\nContainer deployment in Leda Kanto-auto-deployer runs as a one-shot service on boot that goes through the manifest folder (default: /var/containers/manifests) and deploys required containers.\nThe Bitbake recipe for building and installing the auto deployer service can be found at kanto-auto-deployer_git.bb.\nThis recipe also takes all manifests in the kanto-containers directory and installs them in the directory specified by the KANTO_MANIFESTS_DIR BitBake variable (weak default: /var/containers/manifests).\nImportant: To deploy a container in the final Leda image, all you generally need to do is add the manifest in the kanto-containers directory and re-build.\n","categories":"","description":"","excerpt":"TLDR: To deploy a container in the final Leda image, all you generally …","ref":"/leda/docs/dev-and-maintenance/rust/kanto-auto-deployer/","tags":"","title":"Kanto Auto deployer"},{"body":"  Download latest Eclipse Leda release Run Eclipse Leda  on emulated Qemu devices or on Raspberry Pi 4   Configure device, e.g. provision the device Explore the device tools Develop your first Vehicle App using Eclipse Velocitas template Deploy a Vehicle App to the device  ","categories":"","description":"","excerpt":"  Download latest Eclipse Leda release Run Eclipse Leda  on emulated …","ref":"/leda/docs/general-usage/","tags":"","title":"Getting Started"},{"body":"Follow these steps to do a manual device provisioning:\n Log in to Azure Portal Go to Azure Iot Hub Create a new device (Note: Do not create an edge device) Copy the Primary Connection String Create a Kubernetes Secret with the name cloudagent by using kubectl on the device  Alternatively, on command line:\n Install Azure CLI: curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash Allow automated installation of extensions: az config set extension.use_dynamic_install=yes_without_prompt Login to Azure: az login Select the correct subscription: az account set --subscription \u003c\u003csubscription\u003e\u003e Create device: az iot hub device-identity create -n {iothub_name} -d {device_id} Show connection string: az iot hub device-identity connection-string show -n {iothub_name} -d {device_id} -o tsv Create a Kubernetes Secret  ssh -p 2222 root@localhost /usr/local/bin/kubectl create secret generic cloudagent \\ --from-literal=PrimaryConnectionString='\u003c\u003cConnection String\u003e\u003e' Configure credentials for private container registries For each private container registry, a separate Secret is needed. For GitHub, your Personal Access Token requires the read:packages permission.\nssh -p 2222 root@localhost /usr/local/bin/kubectl create secret docker-registry ghcr-io \\ --docker-server=ghcr.io \\ --docker-username=\u003cyour github username\u003e \\ --docker-password=\u003cyour github personal access token\u003e If you have additional project-specific container registries, you may need to add them as well:\nssh -p 2222 root@localhost /usr/local/bin/kubectl create secret docker-registry azurecr \\ --docker-server=\u003cyour cr\u003e.azurecr.io \\ --docker-username=\u003cusername\u003e \\ --docker-password=\u003cpassword\u003e The Pod specifications need to reference the image pull secret:\napiVersion: v1 kind: Pod metadata: name: ... spec: containers: - name: ... image: ... imagePullSecrets: - name: azurecr Verifying correct configuration  Check that the secret cloudagent has been deployed:  root@qemux86-64:~# kubectl get secrets NAME TYPE DATA AGE default-token-pmtd9 kubernetes.io/service-account-token 3 47m cloudagent Opaque 1 10s  Check that the SDV Cloud Agent pod has been deployed and started:  root@qemux86-64:~# kubectl describe pod cloud-connector Name: cloud-connector Namespace: default Priority: 0 Node: qemux86-64/10.0.2.15 ... Status: Running Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True ","categories":"","description":"","excerpt":"Follow these steps to do a manual device provisioning:\n Log in to …","ref":"/leda/docs/device-provisioning/manual-provisioning/","tags":"","title":"Manual Provisioning"},{"body":"Initializing BitBake environment Alternatively, initialize build environment manually. This way, you can invoke multiple build processes in the same context, eg when building individual recipes.\ncd poky source oe-init-build-env ../build-sdv-\u003ctarget\u003e bitbake sdv-image-all leda Individual recipes General usage:\ncd poky source oe-init-build-env ../build-sdv-\u003ctarget\u003e bitbake \u003crecipe\u003e Recipes Structure  meta-leda \\-- classes // Reusable BitBake Classes, eg for offline container image pre-caching \\-- conf // Distribution specific configurations, eg version numbers, release codename \\-- recipes-bsp // Board Support Packages, eg specifics for QEMU and Raspberry Pi \\-- recipes-devtools // Build tools \\-- recipes-kernel // Kernel configuration, eg kernel modules, logging, virtio \\-- recipes-sdv |-- eclipse-kanto // Build recipes for Kanto |-- images // Definition of three disk images: full, minimal, rescue |-- packagegroups // Grouping packages \\-- sdv-base // SDV Base Bundle: k3s, fstab, can0.network |--- base-files |--- SDV Core Utilities |--- DAPR \u0026 HELM CLI \\--- SDV Utilities |-- sdv-containers // Container images recipes for pre-caching |--- Cloud Agent |--- Data Broker |--- Feeder CAN |--- OTel Collector |--- Self Update Agent |--- Vehicle Update manager |--- Example Seat Service \\--- ... |-- sdv-core // SDV Core Bundle |--- SDV Core Bundle Pods // Kubernetes Deployment files |--- DAPR Initializer // Initialize HELM and DAPR on first boot \\--- Cloud Connector // Optional: Native installation of Cloud Agent \\-- tools // Convenience tools for the \"full\" image, eg nerdctl and k9s SDV Base Bundle Contains the recipes to build and install the minimal set of dependencies for the SDV stack on the edge device. With these minimal components, the SDV stack should be able to bootstrap itself.\nExample: bitbake sdv-core-utils\nCAN-Bus Kernel Configuration To enable support for CAN bus related modules, the kernel needs to be reconfigured. This is done by the sdv-canbus-modules.inc include file in the recipes-kernel/linux folder, which patches Poky’s linux-yocto recipe.\nVerifying and displaying the current kernel configuration: bitbake -e virtual/kernel\nTo verify the recipe and the kernel configuration: bitbake linux-yocto -c kernel_configcheck -f\nThe kernel config file can be found in: ./tmp/work/qemux86_64-poky-linux/linux-yocto/*/linux-qemux86_64-standard-build/.config\nSDV Core Bundle Contains the recipes to build and install additional SDV components, which are required for a proper runtime setup.\nExample: bitbake sdv-core-bundle-pods\nSDV Containers Contains the recipes for pre-installing specific containers into the container management at runtime. This is mainly for pre-caching container image layers onto the device to speed up the initial deployment but can also be used to enable offline usecases.\nExample: bitbake sdv-container-seatservice\nBuilding SDV Container Seat Service container image  With dependencies (default): bitbake sdv-container-seatservice Without dependencies: bitbake -v -b ../meta-leda/recipes-sdv/sdv-containers/sdv-container-seatservice.bb Output in /leda-distro/build-sdv-x86_64/tmp/work/core2-64-poky-linux/sdv-container-seatservice/1.0-r0/deploy-rpms/core2_64/sdv-container-seatservice-1.0-r0.core2_64.rpm (Example)  Build Configuration Requirements  Yocto Project 3.4 (honister) or higher - required for k3s support in meta-virtualization 50GB+ free disk space per build configuration Online connection for fetching sources and container images  Build Scripts  build-aarch64-qemu.sh - generix ARM64 for QEMU emulation build-aarch64-rpi4.sh - ARM64 with Raspberry Pi 4 board support package build-arm-qemu.sh - generix ARM for QEMU emulation build-x86_64.sh - generix x64-64 for QEMU emulation  ","categories":"","description":"","excerpt":"Initializing BitBake environment Alternatively, initialize build …","ref":"/leda/docs/build/metalayer/","tags":"","title":"Metalayer"},{"body":"Leda integrates RAUC as a reference implementation and example configuration. It allows the evaluation of the concepts, mechanisms and involved software components in an emulated, virtual environment or on physical devices.\nChecking the RAUC Status Get the current RAUC boot status:\nrauc status  Example output:\nroot@qemux86-64:~# rauc status === System Info === Compatible: Eclipse Leda qemu86-64 Variant: Booted from: rootfs.1 (SDV_B) === Bootloader === Activated: rootfs.1 (SDV_B) === Slot States === o [rootfs.1] (/dev/sda5, ext4, inactive) bootname: SDV_B mounted: / boot status: good x [rootfs.0] (/dev/sda4, ext4, booted) bootname: SDV_A boot status: good Forcing to boot the other slot To manually force the device to boot into another slot, mark the current booted slot as bad, mark the other partitions as active and perform a reboot:\nrauc status mark-bad booted rauc status mark-active other reboot now Testing the rescue system By marking both root slots as bad, the bootloader is supposed to boot the rescue system:\nrauc status mark-bad rootfs.0 rauc status mark-bad rootfs.1 reboot now Example output of rauc:\no [rootfs.1] (/dev/sda5, ext4, inactive) bootname: B boot status: bad o [rootfs.0] (/dev/sda4, ext4, booted) bootname: A mounted: / boot status: bad Customizations The configurations can be customized by applying or patching the following files:\n RAUC Configuration file: meta-leda/recipes-bsp/rauc/files/qemux86-64/system.conf Bootloader Configuration file: meta-leda/recipes-bsp/grub/files/grub.cfg The physical disk partition configuration: meta-leda/recipes-sdv/wic/qemux86-grub-efi.wks  RAUC System Configuration The RAUC System Configuration  is the central configuration of the RAUC Update system.\nExample:\n[system] compatible=Eclipse Leda qemu86-64 bootloader=grub grubenv=/grubenv/grubenv statusfile=/data/rauc.status [keyring] path=ca.cert.pem [slot.efi.0] device=/dev/sda type=boot-gpt-switch region-start=4M region-size=100M [slot.rescue.0] device=/dev/sda3 type=ext4 readonly=true [slot.rootfs.0] device=/dev/sda4 type=ext4 bootname=SDV_A [slot.rootfs.1] device=/dev/sda5 type=ext4 bootname=SDV_B GRUB Bootloader Configuration The GRUB bootloader has a configuration file which describes which partitions are bootable, which partition they are located at and a reference to RAUC’s slot name.\nThe configuration also contains RAUC specific logic and variables required for a proper integration. Please see the full grub.cfg in the source repository and RAUC Documentation - Integration - GRUB for details.\nExcerpt:\n... menuentry \"SDV Slot A (OK=$SDV_A_OK TRY=$SDV_A_TRY)\" { linux (hd0,4)/boot/bzImage root=/dev/vda4 $CMDLINE rauc.slot=SDV_A } menuentry \"SDV Slot B (OK=$SDV_B_OK TRY=$SDV_B_TRY)\" { linux (hd0,5)/boot/bzImage root=/dev/vda5 $CMDLINE rauc.slot=SDV_B } Disk Partitioning with OpenEmbedded Image Creator (WIC) The OpenEmbedded Image Creator is used in BitBake to actually create full disk images with multiple partitions.\nThese disk images are machine specific and the structure of the partitions are configured in OpenEmbedded Kickstart files (*.wks).\nExcerpt qemux86-grub-efi.wks Note: The excerpt is exemplary, please see the sources for a full representation and documentation.\nbootloader --ptable gpt part --fixed-size 50M --source rawcopy --sourceparams=\"file=efi-boot.vfat\" --fstype=vfat --label boot --active part --fixed-size 10M --source rawcopy --sourceparams=\"file=grubenv.vfat\" --fstype=vfat --label grubenv part /rescue --source rootfs --fstype=ext4 --label rescue part / --source rootfs --fstype=ext4 --label root_a part / --source rootfs --fstype=ext4 --label root_b part /data --fixed-size 4G --fstype=ext4 --label data ","categories":"","description":"","excerpt":"Leda integrates RAUC as a reference implementation and example …","ref":"/leda/docs/device-provisioning/self-update/rauc-integration/","tags":"","title":"RAUC Integration"},{"body":"There are multiple variants on how to set up a build environment:\n with GitHub Codespaces - recommended for developers with restricted internet access, such as corporate proxies, or with Windows hosts with VSCode DevContainer - recommended for Linux hosts Custom setup - for teams  ","categories":"","description":"","excerpt":"There are multiple variants on how to set up a build environment: …","ref":"/leda/docs/build/devenv/","tags":"","title":"Setup development environment"},{"body":"The following message flow is an example for the Self Update use case. The message is triggered by command line via Azure IoT Hub.\nsequenceDiagram autonumber actor flops as Fleet Operations participant backend as Digital Twin participant vum as Vehicle Update Manager participant sua as Self Update Agent participant device as Device flops - backend: Rollout Campaign backend -- vum: Device Command Live Message: yamlApply Note left of backend: C2D Message vum -- sua: selfupdate/desiredstate SelfUpdateBundle loop Download sua -- sua: Downloading sua -- vum: selfupdate/desiredstatefeedback vum -- backend: progress end loop Installation sua -- device: Installing Note right of device: RAUC Update Bundle sua -- vum: selfupdate/desiredstatefeedback vum -- backend: progress end sua -- vum: Installed rect rgb(100, 255, 150) sua -- backend: FINISHED_SUCCESS end opt Reboot vum -- device: SysRq Reboot end  Messages The following describes the message flow with example messages in more detail. The following variables are used for dynamic parts of the messages:\n \u003ccuid\u003e - A Correlation ID in form of a UUID \u003cselfUpdateRequestYaml\u003e or \u003cpayload\u003e- The Desired State Self Update Request message in YAML, as defined by the Self Update Agent API \u003chub\u003e - The name or identifier of the message hub \u003cdevice\u003e - The device identifier used by the message hub or other components to identify the device.    Cloud backend sends the Self-Update Request Message as YAML embedded into an Azure IoT Hub C2D Message Envelope:\nPayload:\n{ \"appId\": \"mc-ota-update\", \"cmdName\": \"desiredstate.update\", \"cId\": \"\u003ccuid\u003e\", \"eVer\": \"2.0\", \"pVer\": \"1.0\", \"p\": \u003cselfUpdateRequestYaml\u003e }   Cloud Connector validates envelope and transforms request message into a ContainerOrechestrator message:\nTopic: command//azure.edge:\u003chub\u003e:\u003cdevice\u003e:edge:containers/req/\u003ccuid\u003e/yamlApply\nBody (json):\n { \"topic\": \"azure.edge/\u003chub\u003e:\u003cdevice\u003e:edge:containers/things/live/messages/yamlApply\", \"headers\": { \"content-type\": \"application/json\", \"correlation-id\": \"\u003ccuid\u003e\"}, \"path\": \"/features/ContainerOrchestrator/inbox/messages/yamlApply\", \"value\": { \"correlationId\": \"\u003ccuid\u003e\", \"payload\": \"\u003cpayload\u003e\" } } } Note: Payload (Yaml encoded in JSON) omitted here for clarity, see next step.\n   Vehicle Update manager extracts payload and forward the message to the Self Update Agent message inbox:\nTopic: selfupdate/desiredstate\nMessage:\napiVersion: sdv.eclipse.org/v1 kind: SelfUpdateBundle metadata: name: self-update-bundle-example spec: bundleDownloadUrl: http://leda-bundle-server/sdv-rauc-bundle-qemux86-64.raucb bundleName: swdv-arm64-build42 bundleTarget: base bundleVersion: v1beta3    The Self Update Agent response with status messages during download and installation phases.\nTopic: selfupdate/desiredstatefeedback\nMessage:\napiVersion: sdv.eclipse.org/v1 kind: SelfUpdateBundle metadata: name: \"self-update-bundle-example\" spec: bundleDownloadUrl: \"http://leda-bundle-server/sdv-rauc-bundle-qemux86-64.raucb\" bundleName: \"swdv-arm64-build42\" bundleTarget: base bundleVersion: v1beta3 state: message: Entered Downloading state name: downloading progress: 0 techCode: 0   Once finished, the Vehicle Update Manager will also return a FINISHED_SUCCESS message for the conversation with the backend.\nTopic: e/defaultTenant/azure.edge:\u003chub\u003e:\u003cdevice\u003e:edge:containers\nMessage:\n{ \"topic\": \"azure.edge/\u003chub\u003e:\u003cdevice\u003e:edge:containers/things/twin/commands/modify\", \"headers\": { \"response-required\":false }, \"path\": \"/features/ContainerOrchestrator/properties/status/state\", \"value\": { \"manifest\": [], \"status\": \"FINISHED_SUCCESS\", \"correlationId\":\"\u003ccuid\u003e\" } }   ","categories":"","description":"","excerpt":"The following message flow is an example for the Self Update use case. …","ref":"/leda/docs/device-provisioning/vehicle-update-manager/message-flow/","tags":"","title":"Message Flow"},{"body":"The k8s ecosystem comes with a lot of utilies that allow for the easier management of containers (such as k9s). The kantui util aims to be a “nice” text user interface that lets the user start/stop/remove/get logs of deployed containers in kanto-cm.\nDevelopment notes This tool is again based on the ideas in Communicating with Кanto-CM via gRPC.\nIt spins up two threads - an UI thread (drawing/updating UI) and an IO thread (communicating with kanto-cm via gRPC). The communication between these two threads happens over an async-priority-channel with ListContainers request having a lower priority than Start/Stop/Remove/Get Logs (“user interaction”) requests.\nThis in an “eventually fair” mechanism of communication. That way even if kanto-cm is handling a slow request (such as stopping a container that does not respect SIGTERM) the UI thread is never blocked, allowing for a responsive-feeling UI. The size of the channel is 5 requests and the UI is running at 30 fps. Thus even if the UI gets out-of-sync with the actual state of container management it would be “only” for 5 out 30 frames.\nCursive and ncurses-rs The cursive crate is used as a high level “framework” as it allows very easy handling of UI events via callbacks, though this might be prone to callback hell.\nThe default backend for cursive is ncurses-rs which a very thin Rust wrapper over the standart ncurses library. This in theory would be the optimal backend for our case as ncurses is a very old and stable library that has buffering (other backends lead to flickering of the UI on updates) and is dynamically linked (smaller final binary size).\nThe ncurses-rs wrapper however is not well-suited to cross-compilation as it has a custom build.rs that generates a small C program, compiles it for the target and tries to run it on the host. The only reason for this C program to exist is to check the width of the char type. Obviously, the char type on the host and the target might be of different width and this binary might not even run on the host machine if the host and target architectures are different.\nAfter coming to the conclusion that the ncurses-rs backend was not suitable, kantui was migrated to the termion backend + the cursive_buffered_backend crate which mitigates the flickering issue.\n[dependencies] ... cursive_buffered_backend = \"0.5.0\"  [dependencies.cursive] default-features=false version = \"0.16.2\" features = [\"termion-backend\"] This completely drops the need for ncurses-rs but results in a slightly bigger binary (all statically linked).\nBitbake Recipe The recipe was created following the guidelines in Generating bitbake recipes with cargo-bitbake and can be found in meta-leda/meta-leda-components/recipes-sdv/eclipse-leda/.\nFuture improvement notes   The gRPC channel can get blocked thus effectively “blocking” the IO-thread until it is freed-up again. Maybe open a new channel for each request (slow/resource heavy)?\n  Reorganize the code a bit, move all generic functionally in the lib.rs.\n  ","categories":"","description":"","excerpt":"The k8s ecosystem comes with a lot of utilies that allow for the …","ref":"/leda/docs/dev-and-maintenance/rust/kantui/","tags":"","title":"Kantui"},{"body":"The self update agent (SUA) is a component responsible for the OS Update process. SUA is communicating on MQTT interface via usage of defined messages. Internally, SUA uses RAUC to perform the update.\nFollowing sequence diagram shows the happy path example of communication between components.\nProcess Overview sequenceDiagram participant m as MQTT Broker participant s as SUA participant r as RAUC s -- m: connect loop Wait for message: selfupdate/desiredstate Note left of s: Initial start s - m: selfupdate/currentstate Note left of s: Trigger for OTA m - s: selfupdate/desiredstate s - m: selfupdate/desiredstatefeedback: downloading 0% s - s: download bundle s - m: selfupdate/desiredstatefeedback: downloading 51% s - r: install s - m: selfupdate/desiredstatefeedback: installing 0% r - r: install r - s: share progress: e.g. 51% s - m: selfupdate/desiredstatefeedback: installing 51% r - s: installation ready s - m: selfupdate/desiredstatefeedback: installed s - m: selfupdate/desiredstatefeedback: idle end  MQTT Message Definitions MQTT messages are specified as follows:\nselfupdate/desiredstate    Topic Direction Description     selfupdate/desiredstate IN This message triggers the update process. The payload shall contain all data necessary to obtain the update bundle and to install it.    apiVersion: \"sdv.eclipse.org/v1\" kind: SelfUpdateBundle metadata: name: self-update-bundle-example spec: bundleName: swdv-arm64-build42 bundleVersion: v1beta3 bundleDownloadUrl: https://example.com/repository/base/ bundleTarget: base selfupdate/currentstate    Topic Direction Description     selfupdate/currentstate OUT This message is being sent once, on SUA start. It contains information about currently installed OS version.    apiVersion: \"sdv.eclipse.org/v1\" kind: SelfUpdateBundle metadata: name: self-update-bundle-example spec: bundleVersion: v1beta3 selfupdate/desiredstatefeedback    Topic Direction Description     selfupdate/desiredstatefeedback OUT This message is being sent by SUA to share current progress of triggered update process. This is the OUT counterpart of selfupdate/desiredstate input message.    apiVersion: \"sdv.eclipse.org/v1\" kind: SelfUpdateBundle metadata: name: self-update-bundle-example spec: bundleName: swdv-arm64-build42 bundleVersion: v1beta3 bundleDownloadUrl: https://example.com/repository/base/ bundleTarget: base state: name: \"idle|installing|etc.\" progress: 0|51|99|etc., techCode: 0|1|5|etc., message: \"Cannot download from url|Bundle already installed|etc.\" state enum State name field can have one of following values:\n   State Description Additional payload data     uninitialized When the SUA is not configured yet -   idle Configured and waiting for messages -   downloading Downloading the bundle file progress   installing Performing installation progress   installed Installation process was successful, new OS version is installed on inactive disc Slot. Important: to finish the OTA process, reboot is required, and it shall be performed by another component, such as the Vehicle Update Manager. -   failed Error occurred techCode    techCode values techCode field is providing additional details to the state value. It is especially useful for the failed state, as it can specify the reason of failure.\n   Value Description     0 OK, no error   1001 Download failed   2001 Invalid Bundle   3001 Installation failed   4001 Update rejected, bundle version same as current OS version   5001 Unknown Error    ","categories":"","description":"","excerpt":"The self update agent (SUA) is a component responsible for the OS …","ref":"/leda/docs/device-provisioning/self-update/api/","tags":"","title":"API Reference"},{"body":"After setting up your VSCode DevContainer or GitHub Codespace you can proceed with the actual build process. Here you have two choices - either using the kas-build system or setting up the build manually.\nBuilding with kas This is the easiest way to build leda semi-automatically\n cd /workspaces/meta-leda-fork/ Open the VSCode terminal and run kas build Note: you can alter the build options by modifying the .config.yaml file in the trunk of the repository  Building manually You can also build Leda manually if more customization of the build process is required.\n  export LEDA_WORKDIR=/workspaces/meta-leda-fork/\n  cd ${LEDA_WORKDIR}\n  Clone the Poky repository with the required release, e.g. kirkstone and pull updates if necessary:\ngit clone git://git.yoctoproject.org/poky cd poky git checkout -t origin/kirkstone -b kirkstone git config pull.rebase false git pull   Prepare the build environment:\nsource oe-init-build-env   Dry-run a build of the Linux Kernel recipe using BitBake:\nbitbake --dry-run linux-yocto   Checkout the meta-layer dependencies for Leda:\ncd $LEDA_WORKDIR git clone -b kirkstone https://github.com/rauc/meta-rauc.git meta-rauc git clone -b kirkstone https://github.com/rauc/meta-rauc-community.git meta-rauc-community git clone -b kirkstone https://git.yoctoproject.org/meta-virtualization meta-virtualization git clone -b kirkstone https://git.openembedded.org/meta-openembedded meta-openembedded   Change to the poky/build directory (generated from the oe-init-build-env script automatically)\n  Add all the necessary meta-layers:\nbitbake-layers add-layer ${LEDA_WORKDIR}/meta-rauc bitbake-layers add-layer ${LEDA_WORKDIR}/meta-rauc-community/meta-rauc-qemux86 bitbake-layers add-layer ${LEDA_WORKDIR}/meta-openembedded/meta-oe bitbake-layers add-layer ${LEDA_WORKDIR}/meta-openembedded/meta-filesystems bitbake-layers add-layer ${LEDA_WORKDIR}/meta-openembedded/meta-python bitbake-layers add-layer ${LEDA_WORKDIR}/meta-openembedded/meta-networking bitbake-layers add-layer ${LEDA_WORKDIR}/meta-virtualization bitbake-layers add-layer ${LEDA_WORKDIR}/meta-leda-components bitbake-layers add-layer ${LEDA_WORKDIR}/meta-leda-bsp bitbake-layers add-layer ${LEDA_WORKDIR}/meta-leda-distro   Dry run:\nDISTRO=leda bitbake --dry-run sdv-image-all   Real build:\nDISTRO=leda bitbake sdv-image-all   You can also build one of the target recipies this way:\nDISTRO=leda bitbake kanto-container-management   Note: in this case you can set the target architecture and other build options in the build/local.conf file\n  ","categories":"","description":"","excerpt":"After setting up your VSCode DevContainer or GitHub Codespace you can …","ref":"/leda/docs/build/devenv/build-kas-manually/","tags":"","title":"Building with kas/manually"},{"body":"   Dependency Type License URL     poky OpenEmbedded Metalayer MIT or GPL-2.0-only https://www.yoctoproject.org/software-item/poky/   meta-virtualization OpenEmbedded Metalayer MIT https://git.yoctoproject.org/meta-virtualization   meta-networking OpenEmbedded Metalayer MIT https://git.yoctoproject.org/meta-networking   meta-rauc OpenEmbedded Metalayer MIT https://github.com/rauc/meta-rauc   meta-openembedded OpenEmbedded Metalayer MIT https://git.openembedded.org/meta-openembedded   meta-security OpenEmbedded Metalayer MIT https://git.yoctoproject.org/meta-security   meta-rauc-community OpenEmbedded Metalayer MIT https://github.com/rauc/meta-rauc-community   meta-raspberrypi OpenEmbedded Metalayer MIT https://git.yoctoproject.org/meta-raspberrypi    ","categories":"","description":"","excerpt":"   Dependency Type License URL     poky OpenEmbedded Metalayer MIT or …","ref":"/leda/docs/project-info/dependencies/","tags":"","title":"Dependencies"},{"body":"The device needs to be configured before it can make a connection to the cloud.\nThe following initial configuration steps are required:\n Create a device in Azure IoT Hub Configure authentication on device Configure credentials for accessing private container registries  Create a device in Azure IoT Hub For the device to be connectable, it needs to be known to the cloud service first. In these steps, we will create a new device identity by using Azure IoT Hub.\nPre-Requisites:\n  Virtual device must already be started with runqemu ... or leda\nNote: For Raspberry Pi, please follow the manual steps below and adapt the SSH connection options to the IP of your Raspbery Pi.\n  The virtual device needs to be remotely accessible via ssh port 2222 on the host’s localhost (Qemu port forwarding in userspace) or via ssh port 22 on the IP address 192.168.7.2 (Qemu virtual networking using TAP network interface)\n  The k3s service needs to have started successfully, check with systemctl status k3s\n  A Device has been created in Azure IoT Hub\nNote: Do not create anedgedevice.\n  Script on host system Note: For Linux hosts only. Alternatively, you can perform the steps manually on command line.\nPre-Requisite:\n  Install jq and dialog on your host:\napt-get install jq dialog    Run the provisioning script on your host system\n./provision-device.sh    The script will:\n Remotely connect to the QEMU instance via root@192.168.7.2 Install Azure CLI Ask you to login to Azure using the Device Code Flow Ask which Azure Subscription to use Ask which Azure IoT Hub the device shall connect to Ask which existing device to use or to create a new device Generate and retrieve the connection string secret Deploy the connection string to device as Kubernetes secret  Note: Kubernetes will take some time before reconsidering a redeployment of failed pods. If you want to force a redeployment, use k9s to delete the failed pods. The Deployments will then ensure to recreate the pods with the new configurations.\nWhen finished, continue with\n Deploying a Vehicle App Performing a Self Update  Script on guest system Note: When running the device provisioning from the guest, you need to manually retrieve the device secrets first.\n  Run the provisioning script on the guest system\nsdv-provision    The script will:\n Ask for an Azure IoT device Connection String Ask for credentials to access private container registries (‘imagePullSecret’) Deploy the connection string as Kubernetes secret Deploy the imagePullSecret as Kubernetes secret  Note: Kubernetes will take some time before reconsidering a redeployment of failed pods. If you want to force a redeployment, use k9s to delete the failed pods. The Deployments will then ensure to recreate the pods with the new configurations.\nWhen finished, continue with\n Deploying a Vehicle App Performing a Self Update  ","categories":"","description":"","excerpt":"The device needs to be configured before it can make a connection to …","ref":"/leda/docs/device-provisioning/","tags":"","title":"Device Provisioning"},{"body":"As your Raspberry Pi device might not be reachable via network, you can configure the initial credentials by specifying them as a Kubernetes resource file, copy the files to the FAT32 Boot Partition and then apply them on the offine device.\nPre-Requisites   Online connection to device or physical access to the device\n  Device clock needs to be in sync - Verify with timedatectl or systemctl status systemd-timesyncd\n  Installed kubectl on your workstation - See Installing kubectl\nNote: If you do not have kubectl at hand, an alternative way is to deploy secrets using Kubernetes Configuration files.\n  Secret for Cloud Connector  Put the SD-Card into your card reader  Windows: Look for a new drive in Windows Explorer F: [boot] Linux: Mount the SD Card   Create a new secret file for the Cloud Connection:  $ kubectl create secret generic cloudagent \\ --dry-run=client \\ --from-literal=PrimaryConnectionString=\"{{Connection String}}\" \\ -o yaml \u003e cloudagent-secret.yaml  For each private Container Registry, create an an additional yaml file to specify the credentials:  $ kubectl create secret docker-registry ghcr-io \\ --dry-run=client \\ --docker-server=ghcr.io \\ --docker-username=\"\u003cUSERNAME\u003e\" \\ --docker-password=\"\u003cPASSWORD\u003e\" \\ -o yaml \u003e ghcr-io-secret.yaml  Copy the secret files to the boot partition of the SD Card Boot the SD-Card on the Raspberry Pi Login as root without password on login prompt Apply the Kubernetes configuration specifications:  root@raspberrypi4-64:~# kubectl apply -f /boot/cloudagent-secret.yaml root@raspberrypi4-64:~# kubectl apply -f /boot/ghcr-io-secret.yaml  You may need to restart the pods for the changes to take effect:  root@raspberrypi4-64:~# kubectl delete pod cloud-connector root@raspberrypi4-64:~# kubectl apply -f /var/lib/rancher/k3s/server/manifests/cloud-connector.podspec.yaml   Verify and wait until k3s is started: systemctl status k3s\n  Optional: Check the system health: sdv-health\nNote: The status of some pods and the cloud connector are expected to stay in FAILED status as long as the Device Provisioning steps are not completed.\n  Continue with Deploying a Vehicle App\n  ","categories":"","description":"","excerpt":"As your Raspberry Pi device might not be reachable via network, you …","ref":"/leda/docs/device-provisioning/provisioning-raspi/","tags":"","title":"Provisioning Raspberry Pi"},{"body":"Run the full build To setup the environment and build the Leda image, please refer to: Setup development environment.\nRunning QEMU from existing build  Switch to the build directory: cd poky \u0026\u0026 source oe-init-build-env ../build-sdv-xxx Use runqemu qemux86-64 nographic slirp qemuparams=\"-m 2048\" to execute the image. Replace qemux86-64 with one of the machines listed above.  slirp enables user-networking which does not require root privileges on the host. tun is default but requires setup on the host. nographic disables graphical user interface qemuparams are additional command line parameters for qemu -m 2048 gives 2GB of memory to the qemu instance, required for k3s   Using tap networking  Login as root without password on login prompt, or use ssh root@192.168.7.2 to login remotely to qemu instance   Using slirp networking  Login as root without password on login prompt, or use ssh -p 2222 root@localhost to login remotely to qemu instance   Continue with Device Provisioning  Variations of runqemu command line  Use runqemu ovmf  ovmf will enable the UEFI support for IA32 (x86) and X64 (x86-64) guests, for testing the dual-boot capabilities and SDV Self-Update mechanisms   All other options are now part of the default Leda distribution configuration (see leda-qemu-settings.inc) Continue with Device Provisioning  Running QEMU in the background To start QEMU in the background enter, use nohup and bring the process into the background.\nnohup runqemu qemux86-64 nographic qemuparams=\"-m 2048 -pidfile qemu.pid\" \u0026 The image is then reachable via ssh root@192.168.72 This will write a file qemu.pid in the current directory including the process ID of QEMU. Once done, kill -9 \u003cqemu.pid\u003e kills the process.\nRunning with kas-shell If you’ve chosen to build the Leda image with kas, you can use the kas-shell to run QEMU, with kas setting up the environment for you. To do that change to the main working directory and run:\nkas shell -c 'runqemu slirp nographic ovmf sdv-image-full' ","categories":"","description":"","excerpt":"Run the full build To setup the environment and build the Leda image, …","ref":"/leda/docs/build/run-build/","tags":"","title":"Run the build"},{"body":"Cross Compiling to X86_64 on Ubuntu 20.04 There is currently a step to cross-compile tests to X86_64. In order to successfully run the step, you need to make sure that the following artifacts are available on the runner:\n rustc + cargo: curl https://sh.rustup.rs -sSf | sh docker: follow https://docs.docker.com/engine/install/ubuntu/ and afterwards https://docs.docker.com/engine/install/linux-postinstall/ build-essential: sudo apt-get install build-essential cross (0.1.16): cargo install cross --version 0.1.16 jq: sudo apt-get install jq -y  You may restart your current shell so that all components are available as env vars.\n","categories":"","description":"","excerpt":"Cross Compiling to X86_64 on Ubuntu 20.04 There is currently a step to …","ref":"/leda/docs/build/build-tests/","tags":"","title":"Building tests"},{"body":"","categories":"","description":"","excerpt":"","ref":"/leda/docs/app-deployment/","tags":"","title":"Vehicle Applications"},{"body":"","categories":"","description":"","excerpt":"","ref":"/leda/docs/about/architecture/","tags":"","title":"Architecture"},{"body":"The cloud connector is used in the context of Device Provisioning and cloud backend connectivity.\nOverview Leda Cloud Connector for Azure IoT Hub is a fork (extended and adapted) of the generic Eclipse Kanto’s Azure connector that is being able to process cloud-to-device and device-to-cloud messages as defined for the Software-Defined Vehicle cloud backend.\nRuntime The SDV cloud connector will come up with pluggable architecture that will allow easy transformation of the incoming cloud-to-device command messages (SDV message envelope) to a format suitable and understandable by the rest of the in-vehicle components and vice-versa. It shall be possible to map SDV messages to and from Eclipse Hono and Eclipse Ditto messages using simple configuration, rules written in JSON; thus allowing this component to work together with other Eclipse Kanto components too.\nSource Repository Source Repository: https://github.com/eclipse-leda/leda-contrib-cloud-connector\n","categories":"","description":"","excerpt":"The cloud connector is used in the context of Device Provisioning and …","ref":"/leda/docs/leda-incubator/cloud-connector/","tags":"","title":"Cloud Connector"},{"body":"Thanks for considering to contribute to Eclipse Leda. We really appreciate the time and effort you want to spend helping to improve the project.\nIn order to get you started as fast as possible we need to go through some organizational issues first, though.\nEclipse Development Process This Eclipse Foundation open project is governed by the Eclipse Foundation Development Process and operates under the terms of the Eclipse IP Policy.\n https://eclipse.org/projects/dev_process https://www.eclipse.org/org/documents/Eclipse_IP_Policy.pdf  Eclipse Contributor Agreement Before your contribution can be accepted by the project team contributors must electronically sign the Eclipse Contributor Agreement (ECA).\n http://www.eclipse.org/legal/ECA.php  Commits that are provided by non-committers must have a Signed-off-by field in the footer indicating that the author is aware of the terms by which the contribution has been provided to the project. The non-committer must additionally have an Eclipse Foundation account and must have a signed Eclipse Contributor Agreement (ECA) on file.\nFor more information, please see the Eclipse Committer Handbook: https://www.eclipse.org/projects/handbook/#resources-commit\nMaking Your Changes  Fork the repository on GitHub. Create a new branch for your changes.  Note: When forking multiple repositories (eg most of the time, you also need to make modifications to meta-leda), please use the same branch name of each repository.   Make your changes following the code style guide for the respective type of content:  BitBake Recipes: https://www.openembedded.org/wiki/Styleguide Documentation: https://www.docsy.dev/docs/best-practices/ Shell Scripts (Example Style Guide): https://google.github.io/styleguide/shellguide.html   When you create new files make sure you include a proper license header at the top of the file (see License Header section below). Make sure you include test cases for non-trivial features. Make sure the test suite passes after your changes. Commit your changes into that branch. Use descriptive and meaningful commit messages. Start the first line of the commit message with the a GitHub Issue number if available and a title e.g. [#9865] Add token based authentication. Squash multiple commits that are related to each other semantically into a single one. Make sure you use the -s flag when committing as explained above. Push your changes to your branch in your forked repository. Once you’re satisfied with your contribution, open a Pull Request and Eclipse Leda Committers will start with the review of your changes.  Note: When working with multiple repositories, you need to open separate Pull Requests for each repository.    Adding Documentation to Hugo  Add the markdown document to the appropriate folder in the path leda/content/en. Add the front-matter  ---title:\"title of the file\"date:2022-05-09T13:43:25+05:30--- Additional front matter that can be added –  url : \"specifying a definite url to the file\" weight : 10 (used for ordering your content in lists. Lower weight gets higher precedence.)   The images need to be put in path leda/static/assets. The image reference should be /assets/image.jpg in the markdown file. (Note: Do not use relative paths or url) In case you are creating a new folder, create _index.md file with the front matter only.  Running Locally  Install hugo version 0.98.0 extended Release v0.98.0 · gohugoio/hugo (github.com) Install Docsy theme in the path leda/themes/docsy –  #Run this command from root directory of velocitas-docs git clone https://github.com/google/docsy.git themes/docsy  Install pre-requisites  cd themes/docsy/userguide/ npm install npm install --save-dev postcss  From the leda directory run the command hugo server visit localhost:1313 to see the rendered static site.  Submitting the Changes Submit a pull request via the normal GitHub UI.\nAfter Submitting  Do not use your branch for any other development, otherwise further changes that you make will be visible in the PR.  License Header Please make sure any file you newly create contains a proper license header like this:\n# /******************************************************************************** # * Copyright (c) 2022 Contributors to the Eclipse Foundation # * # * See the NOTICE file(s) distributed with this work for additional # * information regarding copyright ownership. # * # * This program and the accompanying materials are made available under the # * terms of the Apache License 2.0 which is available at # * https://www.apache.org/licenses/LICENSE-2.0 # * # * SPDX-License-Identifier: Apache-2.0 # ********************************************************************************/ You should, of course, adapt this header to use the specific mechanism for comments pertaining to the type of file you create.\nImportant\nPlease do not forget to add your name/organization to the LICENSE file’s Copyright Holders section. If this is not the first contribution you make, then simply update the time period contained in the copyright entry to use the year of your first contribution as the lower boundary and the current year as the upper boundary, e.g.\nCopyright 2017, 2018 ACME Corporation\nBuild  On every PR merge a pipeline run will be triggered. This run will trigger the hugo docs build Hugo v0.98.0 extended is set up for the runner Docsy theme is setup for beautification of static site Then dependencies are installed for the theme Static site is generated and stored in a folder \"public\" The contents of public are committed to gh_pages branch which is exposed to host the github pages  ","categories":"","description":"","excerpt":"Thanks for considering to contribute to Eclipse Leda. We really …","ref":"/leda/docs/project-info/contribution-guidelines/","tags":"","title":"Contribution Guidelines"},{"body":" Manually creating a Release triggers the release workflow. The release workflow calls the build workflow. The build workflow indirectly depends on the sstate cache being prebuilt manually (see optimizations below) The build workflow runs a full build of the SDV-Image-All disk image for all target machines. A separate job is used for each target machine, to ensure an image build for a target machine can finish within 6 hours. Each build contains the creation of SBOM artifacts and the check for CVEs. The SBOM artifacts are in SPDX JSON format and packaged per target machine’s disk image (SDV-Image-Full to include all packages). The OSS License Scanning (using the OSS Review Toolkit) is done asynchronously on a separate fork, as it currently uses a proprietary infrastructure. The ORT-based infrastructure of Eclipse is planned to be used in the future. The web report is attached as a build artifact on the internal fork and not accessible by public currently. Once the build workflow’s jobs are finished, the release workflow will finalize by attaching the release artifacts as assets to the release.  Note: While the build workflow and release workflows are in progress, the GitHub release page of that release does not show any other assets besides the source archives. The release artifacts (eclipse-leda-.tar.xz) will only be visible once all workflows have finished.\nLimitations on standard runners As the GitHub-managed runners are optimized for ephemeral build use cases and a Yocto-based build process is very consuming in regards to CPU and disk capacity, a few optimizations need to be done before being able to run a full build or even a release workflow on limited GitHub-managed standard runners.\nPlease see the documentation about GitHub Hosted Runners for current specs.\n   Resource Standard GitHub Runner Recommended for Yocto     CPU 2-core CPU (x86_64) 16-core CPU (x86_64)   RAM 7 GB of RAM 16 GB of RAM   Disk 14 GB of SSD 128+ GB of SSD   Time max. 6 hours / job not limited    In general, GitHub recommends to split a build process into smaller chunks, which can then fit into the constraints.\nOptimizations The following optimizations have been implemented for the Eclipse Leda public repository and its build workflow:\n  Remote SState Cache: To minimize build time and disk usage, a remote sstate-cache mirror is being used. The mirror is hosted by one of the project sponsors on european Azure infrastructure and available as public HTTP mirror to anonymous Leda builds. The mirror is provided as best-effort, does not provide any kind of service level and may not be available at all times.\nNote: To use the mirror, set the BB_HASHSERVE, MIRROR_SERVER, SSTATE_MIRRORS and related configuration settings. See the mirrors.yaml for a full sample.\n  Prebuilding: To fill the remote sstate cache mirror, another build infrastructure is being used. The repository fork has been configured with additional credentials to authenticate against the remote mirror for uploading the built packages. To ensure these steps are not run on the public OSS repository, the workflow steps use additional conditions to check for the owner of the repository. This is a workaround due to a known issue on GitHub Actions.\n  Chunking of the build steps: To minimize bandwidth transfer, a local GitHub Action Cache is being used. This cache is per target machine and filled with a separate workflow. The Prebuild sstate build jobs will run the BitBake process for 4 hours and then gracefully shut down. The build process will finish the current tasks. The remaining time (max. runtime for GitHub Runners is 6 hours) is used to package and upload the packages to the cache. If the 4 hours of build time are not enough, it may be required to re-run the same job more often.\nNote: The disadvantage of this approach is that each run requires a significant lead time where the remote cache is downloaded, the recipes get parsed again, the build task dependencies are compiled etc. On a persistent runner, this time can be spared.\n  Rerun on sstate miss: When BitBake is missing a package in the sstate mirror (it may exist in the Hash Equivalence Server though), BitBake will log an Error and continue to run the recipe. However, as the cache-miss is logged as error, BitBake will exit with an error code, indicating a failed build, which in turn would mark the GitHub Job as failed, too. To circumvent this problem, the BitBake build process is executed in a loop (max. 3 retries) to ensure that with the current state, all packages can be built without errors, eventually succeeding with the buid.\n  Always upload GitHub Cache: Under normal circumstances, the GitHub Cache action will update the cache on success of the build job - to not poison the cache with failed builds. However, as the Leda build workflows run for a very long time and may fail due to other reasons, the goal is to still reuse the sstate-cache as much as possible. For that reason, the pat-s/always-upload-cache GitHub action is being used, as it will also upload the cache on failed builds.\n  ","categories":"","description":"","excerpt":" Manually creating a Release triggers the release workflow. The …","ref":"/leda/docs/build/github/","tags":"","title":"GitHub Workflow"},{"body":"Download eclipse-leda-logo.zip\nFor Screen White background 2182x794, white, transparent   Black background 2182x794, black, opaque Credits: prothesis / kanellos @ 99designs.com | Font: https://www.dafont.com/olney.font\n  916x916, black   For Print    File File Format Color model     eclipse-leda-logo_01_cmyk.ai Adobe Illustrator CMYK, Black background   eclipse-leda-logo_01_cmyk.eps Encapsulated PostScript CMYK, Black background   eclipse-leda-logo_01_cmyk.pdf Portable Document Format CMYK, Black background   eclipse-leda-logo_01_rgb.ai Adobe Illustrator RGB, Black background   eclipse-leda-logo_01_rgb.jpg JPEG RGB, Black background, 2480x2480   eclipse-leda-logo_01_rgb.png PNG RGB, Black background, 2480x2480   eclipse-leda-logo_02_cmyk.ai Adobe Illustrator CMYK, Transparent background   eclipse-leda-logo_02_cmyk.eps Encapsulated PostScript CMYK, Transparent background   eclipse-leda-logo_02_cmyk.pdf Portable Document Format CMYK, Transparent background   eclipse-leda-logo_02_rgb.ai Adobe Illustrator RGB, Transparent background   eclipse-leda-logo_02_rgb.jpg JPEG RGB, White background, 2480x2480   eclipse-leda-logo_02_rgb.png PNG RGB, Transparent background, 2480x2480   eclipse-leda-logo_03_cmyk_cropped.svg Scalable Vector Graphics CMYK, Transparent background, Cropped   eclipse-leda-logo_03_cmyk.svg Scalable Vector Graphics CMYK, Transparent background    Colors  Primary Color: #6daed1 Secondary Color: #804096  ","categories":"","description":"","excerpt":"Download eclipse-leda-logo.zip\nFor Screen White background 2182x794, …","ref":"/leda/docs/project-info/logo/","tags":"","title":"Project Logo"},{"body":"QEMU’s command line option can get quiet complex. Yocto is providing a convenient wrapper script called runqemu, which takes configurations into account which have been populated during the build process and also takes care of TAP networking setup and teardown. Unfortunately, it can only be used within an OpenEmbedded build environment.\nRunning without runqemu: when you need more control over qemu options\nqemu-system-x86_64 -kernel .\\bzImage-qemux86-64.bin -nographic -m 2G -append \"console=ttyS0 root=/dev/vda1\" -drive file=.../sdv-image-full-qemux86-64.ext4 Running with runqemu: simple and some convenient flags are supported\nrunqemu ovmf sdv-image-full  Running with leda: no options, runs the default settings only\nleda  Enabling KVM for better performance The performance of qemux86_64 is much better when KVM can be used. Try running with:\nrunqemu ovmf kvm  Note: You may need to set up KVM for your host machine first, please refer to How to enable KVM for Poky qemu\n/workspaces/leda-distro/build-sdv-x86_64 (imageformatcleanup ✗) $ ls -al /dev/kvm crw-rw---- 1 root kvm 10, 232 May 16 06:46 /dev/kvm ","categories":"","description":"","excerpt":"QEMU’s command line option can get quiet complex. Yocto is providing a …","ref":"/leda/docs/build/runqemu/","tags":"","title":"Runqemu"},{"body":"The Vehicle Update Manager delegates two different types of updates:\n The Desired State on the Kubernetes layer The Self Update on operating system layer  Desired State The Desired State is applied at runtime on the Kubernetes cluster layer.\nThis type of update mechanism can update vehicle applications, vehicle services and other containers together with configuration resources or data files at runtime. If the applications support it, the rollout can also use high-availability strategies, such as rolling deployments.\nSelf Update The Self Update is applied on reboot of the device only.\nThis type of update mechanism is used for system-level updates which require the operating system to be rebooted to take effect.\n","categories":"","description":"","excerpt":"The Vehicle Update Manager delegates two different types of updates: …","ref":"/leda/docs/device-provisioning/vehicle-update-manager/","tags":"","title":"Vehicle Update Manager"},{"body":"The Leda Incubator project is a place for new software-defined-vehicle related software components which are in incubation state.\nThe goal of the Eclipse Leda Incubator is to foster collaboration in the Eclipse Software Defined Vehicle (SDV) ecosystem, and hopefully results in work proceeding to contribution by their original authors into upstream projects. The Eclipse Leda Incubator would not make any releases itself.\nThe project hosts software-defined vehicle components which are fitting into the general Eclipse Leda scope, but do not fit to either being part of Leda itself, which should be upstreamed into other projects in the future, or which are still highly experimental and should not be considered for any kind of production use.\n","categories":"","description":"","excerpt":"The Leda Incubator project is a place for new software-defined-vehicle …","ref":"/leda/docs/leda-incubator/","tags":"","title":"Incubator"},{"body":"In general, the self-update mechanism for operating system level updates is done with two separate partitions. While one partition is the actively booted partition and in use, the other partition can be updated by writing a partition image to it, as it is unused or inactive.\nOnce the download and writing is done, a reboot is triggered and the boot loader will now switch to the newly updated partition.\nIf the booting of the updated partition fails, the self update mechanism can revert back to the previous partition or boot to a rescue partition.\nAs updating the running operating system cannot be done at runtime, the approach requires additional disk space, a second partition and also requires the device to be rebooted for the updates to take effect.\nIn a vehicle, the self-updater cannot decide on its own when to do a reboot, as the vehicle must be in a safe condition (eg parked, state of charge etc.). Hence, the trigger for performaing the switch to another slot and a subsequent reboot is handed over to a higher level component, such as the vehicle update manager, which may in turn rely on driver feedback or other conditions.\nImplementation with RAUC Update Service RAUC is a lightweight update client that runs on your embedded device and reliably controls the procedure of updating your device with a new firmware revision.\nFor general usage of the RAUC tool, please see the RAUC User manual\nReference configuration The project contains an example reference implementation and configuration using RAUC, which allows the evaluation of the concepts, mechanisms and involved software components in an emulated, virtual environment.\nThe Leda quickstart image contains the following disk partitions:\n a small rescue partition a full SDV installation with a Kubernetes Control Plane (Server + Agent as single node), pre-loaded SDV container images and deployment specifications and additional developer tools such as nerdctl and k9s. a minimal SDV installation with a Kubernetes Control Plane (Server + Agent as single node), but no additional examples or developer tools. This partition is used to demonstrate the self-update functionality. additional boot and data partitions for keeping system state information  *Note: All three rootfs partitions (rootfs) initially contain the same identical copies of the base operating system. Both SDV Root partitions which contain the Kubernetes Control Plane will use the same shared data partition for the Kubernetes Cluster information. *\n","categories":"","description":"","excerpt":"In general, the self-update mechanism for operating system level …","ref":"/leda/docs/device-provisioning/self-update/","tags":"","title":"Self Updates"},{"body":"Running BitBake to build your own images requires some extra setup on the build machine. Please see the following chapters for more information about the build process itself and how to setup a development and build infrastructure.\nIf you are interested to contribute or to get in touch with us, please see our Community pages and Contribution Guidelines. For reporting security vulnerabilities, please follow our Security Policy.\n","categories":"","description":"","excerpt":"Running BitBake to build your own images requires some extra setup on …","ref":"/leda/docs/build/","tags":"","title":"Building Leda"},{"body":"","categories":"","description":"","excerpt":"","ref":"/leda/docs/build/misc/","tags":"","title":"Miscellaneous"},{"body":"Your privacy is important to us. The following Information is to provide you with all information relevant to data protection in order to be able to use the software, in a data protection compliant manner. It is provided as an information source for your solution-specific data protection and data privacy topics. This is not intended to provide and should not be relied on for legal advice.\nYour Role First things first: when you choose and use our software, you are most likely acting in the role of data controller, if personal related data is being processed. Therefore, you must ensure that the processing of personal data complies with the respective local legal requirements, e.g. when processing data within the scope of General Data Protection Regulation (GDPR) the legal requirements for a controller from the GDPR.\nWhere may the processing of personal related data be relevant? When using our software in combination with other software components, personal data or data categories may be collected for the purpose of developing, testing and running in-vehicle applications (Vehicle Apps). Possible examples are the vehicle identification number (VIN), the number plate, GPS data, video data, audio data, or other measurement data. You can determine which data or data categories are collected when configuring the software. These data are stored in volatile memory and are deleted by shutting down the system. You are responsible for the compliant handling of the data in accordance with the applicable local law.\nWhat have we done to make the software data protection friendly? This section describes the measures taken to integrate the requirements of the data protection directly into the software development. The technical measures described below follow a “privacy by design” approach.\n  Local data: The software may save data permanently in local virtual storage (eg when run in QEMU Emulator) or on local physical storage (SD-Card on Raspberry PI). All collected or processed data can be deleted by either deleting the virtual storage file (*.qcow2), or by erasing the SD-Card.\n  Cloud storage: The software may send data to cloud endpoints controlled by you or your organization. Examples include connectivity data, device identification, device health, device telemetry, application metrics and application logs. Collection and processing of example data on the device is enabled by default. Sending of device data to cloud endpoints must be explicitly enabled by performing the device provisioning process. The actual cloud endpoints are determined and configured during the device provisioning process. All collected or processed data can be deleted on the cloud side in the respective cloud endpoints.\n  Vulnerabilities: The release process for this software is set up to always update to the newest package updates. The project will continously release new versions of the software. To protect personal data, it is advisable to always use the latest version of the software.\n  Important: When you use the Eclipse Leda quickstart images for non-volatile setups, it is essential to reconfigure the system and harden it, this includes but is not limited to the following configuration items:\n Disable system user (root) password and login Disable SSH login with password Adding a new Linux user with restricted permissions Adding SSH key based authentication Kubernetes Resources: Secrets, such as Device Identity Certificates for Cloud Connection and Access credentials for private Container Registries    ","categories":"","description":"","excerpt":"Your privacy is important to us. The following Information is to …","ref":"/leda/docs/project-info/privacy-information/","tags":"","title":"Privacy Information"},{"body":"The Leda teams provides some custom utilities that allow for a more integrated end-user experience. They can be found in the main leda-utils repository on GitHub: eclipse-leda/leda-utils.\nThe following pages are meant to serve as both internal documentation and general guidelines when developing for Leda.\nBash Leda uses bash as its main shell, thus all scripts should be bash-compatible (use the #!/bin/bash shebang). As a Poky+OE-based distro we use BusyBox for core-utils.\nUtility-Specific Pages\nThe bash-based leda-utils are all deployed with the same recipe under the sdv-base packagegroup: meta-leda/meta-leda-components/recipes-sdv/sdv-base.\nRust The current main branch for leda-distro (and meta-leda) is based on the Kirkstone Yocto/Poky release.\nToolchain version The version of the Rust toolchain available in OE (Kirkstone) is 1.59.0. Make sure to target 1.59.0 (or earlier) when developing Rust-based utils for leda. To set 1.59.0 as your default Rust version on your development machine:\n$ rustup install 1.59.0 $ rustup default 1.59.0 $ cargo --version cargo 1.59.0 (49d8809dc 2022-02-10) Generating bitbake recipes with cargo-bitbake After making sure your toolchain is on version 1.59.0 go trough a clean build of your Rust binary:\n$ cd \u003crust_project_dir\u003e $ cargo clean $ rm Cargo.lock $ cargo build --release This will make sure the Cargo.lock is re-generated with packages matching the Rust version. The cargo.bbclass on which Rust recipes are based, requires all Rust crates + their version (matching the Cargo.toml) to be specified as a “SRC_URI +=”. This can become tedious and error-prone if done by hand. That’s why meta-rust provides a tool called cargo-bitbake that generates a minimal recipe with all the crates it finds in the Cargo.lock files of the project.\n$ cargo install --locked cargo-bitbake $ cd \u003crust_project_dir\u003e $ cargo bitbake This will generate a recipe in the project root which you can use as a starting point.\nExample: kantui_git.bb\nNote this recipe will only build your Rust crate. To deploy/install your binary you have to define a .inc file with the same name as the recipe that would handle the installation.\nExample: kantui.inc\nKnown bugs The built-in proc_macros crate is not being imported properly by meta-rust (Kirkstone), thus breaking all library crates that define a proc_macro (meta-rust issue 266). To fix this create a libstd-rs_%.bbappend file containing the single line:\nS = \"${RUSTSRC}/library/test\" meta-leda already provides this fix here, so it should not be necessary to implement it again.\n“Fat” Link time optimization LTO is a nice feature of LLVM that can optimize even through language boundaries at link-time, but leads to longer overall build times. That is why Rust by default uses “thin” LTO which may result in larger/slower binaries. “Fat” LTO can be enabled when building release binaries by adding to the Cargo.toml file the following section:\n[profile.release] lto = true For kantui this leads to reduction of the final binary size from ~8 MiB to ~5 MiB.\nMore information on profile-optimizations can be found here.\nNote: Stripping the debug symbols completely results in further binary size reduction, but BitBake fails with a QA problem when deploying stripped binaries.\nRust-based utilities Utility-Specific Pages\n","categories":"","description":"","excerpt":"The Leda teams provides some custom utilities that allow for a more …","ref":"/leda/docs/dev-and-maintenance/","tags":"","title":"Developing and Maintaining Utils"},{"body":"This cheat sheet gives you an overview of common command line commands to interact with the tools available on the quickstart image.\n   Category Task Command     General Overall info sdv-health    Show device info sdv-device-info    Device provisioning sdv-provision    Switch Keyboard layout loadkeys de   Kubernetes Check k3s status systemctl status k3s    k9s k9s    k3s service logs journalctl -f -l -t k3s    Show all Pods kubectl get pods -A    Pod details kubectl describe pod \u003cpodname\u003e    Pod logs kubectl logs \u003cpodname\u003e \u003ccontainername\u003e    Auto deployments See /var/lib/rancher/k3s/server/manifests/    Delete a Pod kubectl delete pod \u003cpodname\u003e    Redeplod a Pod kubectl apply -f \u003cpodspec.yaml\u003e    Shell in a Pod kubectl exec --stdin --tty \u003cpodname\u003e -- /bin/sh    Cluster IP kubectl get svc clusterip   ContainerdD Show images ctr --address /run/k3s/containerd/containerd.sock --namespace=k8s.io i ls    Import local archive ctr --address /run/k3s/containerd/containerd.sock --namespace=k8s.io i import \u003cdocker.tar\u003e   DAPR Reinstall dapr-init.sh   Mosquitto Show all messages mosquitto_sub -v -t '#' -h \u003cMosquittoClusterIP\u003e'    Send message mosquitto_pub -t '\u003ctarget/topic\u003e' -h \u003cMosquittoClusterIP\u003e -m '{\"foo\":\"bar\"}'    Mosquitto Cluster IP kubectl get svc mosquitto   Cloud Connectivity Cloud agent logs kubectl get logs sdv-core-cloud-agent    Connectivity status mosquitto_rr --quiet -h \u003cMosquittoClusterIP\u003e -t 'edge/thing/request' -e 'edge/thing/response' -m ''   RAUC Self Update Current boot status rauc status    Switch to other boot slot rauc status mark-active other    ","categories":"","description":"","excerpt":"This cheat sheet gives you an overview of common command line commands …","ref":"/leda/docs/general-usage/cheatsheet/","tags":"","title":"Cheatsheet"},{"body":"In order to verify that the device and the cloud connector can successfully receive messages sent by the cloud backend, we will send a dummy message to the device.\nPre-Requisites  Device is up and running, e.g by running in qemu Eclipse Leda has successfully booted The device has been provisioned and configured, see Device Provisioning  Validating configuration First, let’s check that the cloud connection is active.\n Login as root Run sdv-health and check for the SDV Connectivity section:  sdv-health  Start watching the output of the cloud connector:  kubectl logs cloud-connector -f  Note: When an unknown type of message is received, the cloud connector will log an error:\n2022/04/13 16:04:41.911727 [agent] ERROR Handler returned error err=\"cannot deserialize cloud message: invalid character 'H' looking for beginning of value   Start watching on the MQTT message broker:  MOSQUITTO_HOST=$(kubectl get service/mosquitto -o jsonpath='{.spec.clusterIP}' --request-timeout='60s') mosquitto_sub -h ${MOSQUITTO_HOST} -t '#' --pretty -v  Note: When a known type of message is received, the cloud connector will forward the message to the MQTT broker into the corresponding topic $appId/$cmdName\n Sending a Device Message  Go to the Web Console of Azure IoT Hub Select the device Click on “Send Message” Enter a C2D payload and click “Send”  Alternatively, on command line, use the Azure CLI client. Replace DeviceId and IotHubName with the appropriate names of your IoT Hub and device.\naz iot device c2d-message send \\ --device-id ${DeviceID} \\ --hub-name ${IotHubName} \\ --data 'Hello World' ","categories":"","description":"","excerpt":"In order to verify that the device and the cloud connector can …","ref":"/leda/docs/app-deployment/cloud2device-messages/","tags":"","title":"Cloud2Device Messages"},{"body":"Building private components Some components are not yet released to the public.\nTo build them, or to integrate them to the image, these components need to be manually downloaded first.\nsdv-databroker-cli  Install GitHub CLI Add to local.conf to include in image:  CORE_IMAGE_EXTRA_INSTALL += \" sdv-databroker-cli\"  Manually login to GitHub CLI   unset GITHUB_TOKEN gh auth login Output:  ? What account do you want to log into? GitHub.com ? You're already logged into github.com. Do you want to re-authenticate? Yes ? What is your preferred protocol for Git operations? HTTPS ? How would you like to authenticate GitHub CLI? Login with a web browser ! First copy your one-time code: A123-C12B Press Enter to open github.com in your browser... ✓ Authentication complete. - gh config set -h github.com git_protocol https ✓ Configured git protocol ✓ Logged in as johndoe  Manually download the release artifacts:   $ gh release download v0.14.0 \\ --archive=zip \\ --repo eclipse-leda/incubator-vehicleapi \\ --dir downloads $ gh release download v0.14.0 \\ --pattern 'bin_release_databroker_*.tar.gz' \\ --repo eclipse-leda/incubator-vehicleapi \\ --dir downloads  Build the recipe:  cd poky source oe-init-build-env ../build-sdv-x86_64 bitbake sdv-databroker-cli  Build the image:  bitbake sdv-image-minimal ","categories":"","description":"","excerpt":"Building private components Some components are not yet released to …","ref":"/leda/docs/build/sdv-private-components/","tags":"","title":"Incubation Components"},{"body":"This project implements the Eclipse Foundation Security Policy\n https://www.eclipse.org/security  Reporting a Vulnerability Please report vulnerabilities to the Eclipse Foundation Security Team at security@eclipse.org\nSupported Yocto Versions    Version Supported     Yocto 4.x (Kirkstone) Yes   Yocto 3.4 (Honister) EOL   Yocto 3.3 Untested   Yocto \u003c 3.3 No    Important: When you use the quickstart images for non-volatile setups, it is essential to reconfigure the system and harden it.\nConfiguration Items  Disable system user (root) password and login Disable SSH login with password Adding a new Linux user with restricted permissions Adding SSH key based authentication Kubernetes Resources: Secrets  Device Identity Certificates for Cloud Connection Access credentials for private Container Registries    Device Identity for Cloud Connector    Method Implementation Intended use     Pre-Shared Symmetric Key Azure IoT Hub Connection String Development   Certificates X.509 Certificates Production    Kubernetes Secrets Encryption    Method Implementation Intended use     Plain text Base64 Encoding Development   Encrypted Kubernetes: Encrypting Secret Data at Rest Production    ","categories":"","description":"","excerpt":"This project implements the Eclipse Foundation Security Policy …","ref":"/leda/docs/project-info/security/","tags":"","title":"Security Policy"},{"body":"The self update agent is used in the context of OTA Self Updates.\nOverview The Self Update Agent (SUA) as part of the Eclipse Leda Incubator project proposal is a software component responsible for performing updates of system-level components of the connectivity device, such as\n Boot Loader Operating System Device Firmware Hardware Drivers … other parts of the system, which cannot be deployed as containerized packages or may require a reboot of the device.  Implementation and Deployment SUA is using the RAUC framework via D-Bus calls, but it is designed in a way that switching to other updating solution shall be possible. SUA may be controlled by an external higher-level orchestration component via defined MQTT messages, which carry necessary for update data, such as version and URL of the update bundle. Update process feedback and the end result are also communicated via defined MQTT messages. Software Update Agent is implemented in C++ and configured to be running inside of a container.\nSource Repository Source Repository: https://github.com/eclipse-leda/leda-contrib-self-update-agent\n","categories":"","description":"","excerpt":"The self update agent is used in the context of OTA Self Updates. …","ref":"/leda/docs/leda-incubator/self-update-agent/","tags":"","title":"Self Update Agent"},{"body":"The vehicle update manager is used in the context of OTA Software Updates.\nOverview Vehicle Update Manager (VUM) is an extended version of the Eclipse Kanto Container Manager that is being able to handle new desired state for the software on the whole vehicle device.\nImplementation The desired state comes as a multi document YAML content and it includes a list of Kubernetes resources:\n Deployments Pods Services ConfigMaps Custom Resources  System-Level Updates    VUM detects the system-level update custom resource and passes it for further processing to the Self Update Agent.\nThe remaining resources are forwarded to a Kubernetes-compatible control plane and handled like the well-known kubectl command - creating new resources, updating existing ones or deleting old ones that are no longer present in the desired state manifest.\nVUM also monitors the self-update agent and the control plane, and compiles and report the current state of the device, again as a list of Kubernetes resources.\nSource Repository Source Repository: https://github.com/eclipse-leda/leda-contrib-vehicle-update-manager\n","categories":"","description":"","excerpt":"The vehicle update manager is used in the context of OTA Software …","ref":"/leda/docs/leda-incubator/vehicle-update-manager/","tags":"","title":"Vehicle Update Manager"},{"body":"","categories":"","description":"","excerpt":"","ref":"/leda/docs/project-info/","tags":"","title":"Project Information"},{"body":"Versioning and Codenames  Distribution Versioning is following the x.y.z syntax for the distribution, e.g. Leda 0.0.1 Build Versioning is following the git describe syntax: v0.0.1-blank-168-g7e14c4c  Latest tag name: v0.0.1 Branch name: blank Number of commits behind: 168 Short Commit ID: g7e14c4c   Codenames are taken from Wikipedia List of Motor Racing Tracks Initial codename is Hockenheim Current distribution version can be taken from /etc/issue:  root@qemux86-64:~# cat /etc/issue Eclipse Leda v0.0.8 How to Release A new release is triggered manually from its GitHub web page Releases section. By clicking on the Draft new release button, the release process starts:\n Select a branch to use as code base for the release Create a tag using the standard pattern vX.Y.Z Write a title Release X.Y.Z and release notes Optionally select if this is a pre-release Publish the release Validate the release  With Publish the release action the release workflow located in .github/workflows/release.yml will be triggered. This will start building the distro image files for the supported platforms, running the test cases and generating reports as junit xml and license scanning. If the image generation and the test runs are successful the artifacts: images, test binaries and qa reports will be attached as assets to the release.\nThe build (build.yml) and release (release.yml) workflows share a common reusable workflow (base.yml). In this way the release workflow repeats the build actions without duplicating the code.\nDetailed build information on device Eclipse Leda Version root@qemux86-64:~# cat /etc/issue Eclipse Leda v0.0.8 Exact Build Timestamp root@qemux86-64:~# cat /etc/version 20220408135014 root@qemux86-64:~# cat /etc/timestamp 20220408135230 Details of Build Information root@qemux86-64:~# cat /etc/build ----------------------- Build Configuration: | ----------------------- DISTRO = leda DISTRO_VERSION = 2022 DATETIME = 20220408135014 DISTRO_NAME = Eclipse Leda IMAGE_BASENAME = core-image-minimal MACHINE = qemux86-64 TUNE_PKGARCH = core2-64 MACHINE_FEATURES = alsa bluetooth usbgadget screen vfat x86 pci rtc qemu-usermode DISTRO_FEATURES = acl alsa argp debuginfod ipv4 ipv6 largefile pcmcia usbgadget usbhost wifi xattr zeroconf pci vfat seccomp largefile ptest multiarch vulkan virtualization k8s seccomp raucg COMMON_FEATURES = IMAGE_FEATURES = debug-tweaks TUNE_FEATURES = m64 core2 TARGET_FPU = APP_URI_PREFIX = APP_URI_BRANCH = ----------------------- Layer Revisions: | ----------------------- meta = honister:ee68ae307fd951b9de6b31dc6713ea29186b7749 meta-poky = honister:ee68ae307fd951b9de6b31dc6713ea29186b7749 meta-yocto-bsp = honister:ee68ae307fd951b9de6b31dc6713ea29186b7749 meta-leda = main:30a5ff0a7e04dfa2c9b43175a49ac7a2ae0c64a9 -- modified meta-rauc = honister:3faf4cc4fcf558e99dad5aa8532fef2ecd566653 meta-filesystems = honister:061b7fc74f887454251307ef119b808a90654d3f meta-networking = honister:061b7fc74f887454251307ef119b808a90654d3f meta-oe = honister:061b7fc74f887454251307ef119b808a90654d3f meta-python = honister:061b7fc74f887454251307ef119b808a90654d3f meta-perl = honister:061b7fc74f887454251307ef119b808a90654d3f meta-virtualization = honister:bd7511c53b921c9ce4ba2fdb42778ca194ebc3e8 meta-security = honister:fb77606aef461910db4836bad94d75758cc2688c patch = main:a041dad5be9444d55491b57cb6a669a44196566d -- modified ","categories":"","description":"","excerpt":"Versioning and Codenames  Distribution Versioning is following the …","ref":"/leda/docs/build/release/","tags":"","title":"Releasing"},{"body":"The Eclipse Leda project provides system image “recipes” to deliver a functional and always-available Linux-based image/distribution in the context of SDV (Software Defined Vehicle), by pulling together individual contributor pieces from Eclipse SDV and the larger OSS community.\nThe quickstart images help to learn how the SDV development, test and deployment lifecycle works from an E2E perspective, including the deployment of applications into the container runtimes on constrained embedded devices.\nThe ready images are also useful for quickly setting up showcases with virtual or real hardware devices.\nEclipse Leda provides a Poky-based reference build pipeline and an OpenEmbedded Metalayer meta-leda for integration into existing Yocto-based projects.\nUsage  Download latest Eclipse Leda release Run Eclipse Leda  on emulated Qemu devices or on Raspberry Pi 4   Configure device, e.g. provision the device Explore the device tools Develop your first Vehicle App using Eclipse Velocitas template Deploy a Vehicle App to the device  Supported Machines / Build Configurations\n Emulated Qemu: x86-64, ARM64, ARM Raspberry Pi 4  Introduction Video   Components Overview Features  Publish/Subscribe infrastructure for local messaging and cloud connectivity Linux lightweight container runtime and Kubernetes control plane Vehicle Update Manager to orchestrate deployments of Vehicle Applications over the air (SOTA) Self Update Agent for firmware-over-the-air (FOTA) updates Example Vehicle Seat Service implementation Metrics and logs collector for Vehicle Apps  See About - Features for more details about current implementation and About - Roadmap for our future work.\nLicense and Copyright This program and the accompanying materials are made available under the terms of the Apache License 2.0 which is available at https://www.apache.org/licenses/LICENSE-2.0\nFor details, please see our license NOTICE\n","categories":"","description":"","excerpt":"The Eclipse Leda project provides system image “recipes” to deliver a …","ref":"/leda/docs/","tags":"","title":"Documentation"},{"body":"For local deployment without a cloud backend, deployment specifications for Eclipse Velocitas Vehicle Apps may be put directly onto the device.\n Create a Kubernetes deployment specification for your vehicle app Copy the specification file to the device, e.g. scp -P 2222 myapp.yaml root@localhost Apply the specification: ssh -p 2222 root@localhost /usr/local/bin/kubectl apply -f myapp.yaml  Example Pod Specification apiVersion:v1kind:Podmetadata:name:my-example-appannotations:dapr.io/enabled:\"true\"dapr.io/app-id:\"my-example-app\"dapr.io/app-port:\"50008\"dapr.io/log-level:\"info\"dapr.io/app-protocol:\"grpc\"spec:restartPolicy:AlwayshostAliases:- ip:\"10.0.2.15\"hostnames:- \"edgehost\"containers:- name:my-example-appimage:my-private-container-registry.azurecr.io/my-example-app:latestimagePullPolicy:IfNotPresentports:- containerPort:50008env:- name:SEAT_SERVICE_ADDRESSvalue:\"edgehost:50051\"imagePullSecrets:- name:my-private-container-registry-secret","categories":"","description":"","excerpt":"For local deployment without a cloud backend, deployment …","ref":"/leda/docs/app-deployment/velocitas/","tags":"","title":"Velocitas VApps"},{"body":" Download latest Eclipse Leda release Run Eclipse Leda  on emulated Qemu devices or on Raspberry Pi 4   Configure device, e.g. provision the device Explore the device tools Develop your first Vehicle App using Eclipse Velocitas template Deploy a Vehicle App to the device  ","categories":"","description":"","excerpt":" Download latest Eclipse Leda release Run Eclipse Leda  on emulated …","ref":"/leda/docs/__shared/usage-overview/","tags":"","title":""},{"body":"","categories":"","description":"","excerpt":"","ref":"/leda/categories/","tags":"","title":"Categories"},{"body":"  .mb-4 { margin-bottom: 0.5rem !important; } .mb-5 { margin-bottom: 2.0rem !important; }   #td-cover-block-0 { background-image: url(/leda/logo_01_rgb-background_hueacd1e1ea47385f7b501b6766c7efb27_129233_960x540_fill_catmullrom_center_3.png); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/leda/logo_01_rgb-background_hueacd1e1ea47385f7b501b6766c7efb27_129233_1920x1080_fill_catmullrom_center_3.png); } }  Eclipse Leda - Quickstart images for software-defined vehicle development Get started with Eclipse Leda         ","categories":"","description":"","excerpt":"  .mb-4 { margin-bottom: 0.5rem !important; } .mb-5 { margin-bottom: …","ref":"/leda/","tags":"","title":"Leda"},{"body":"","categories":"","description":"","excerpt":"","ref":"/leda/tags/","tags":"","title":"Tags"}]